<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lizhenzhublog.github.io/HTML</id>
    <title>Li Zhenzhu, Ph.D</title>
    <updated>2020-06-22T15:52:11.662Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lizhenzhublog.github.io/HTML"/>
    <link rel="self" href="https://lizhenzhublog.github.io/HTML/atom.xml"/>
    <subtitle>Binzhou Medical University Hospital, Email: timeanddoctor@gmail.com.</subtitle>
    <logo>https://lizhenzhublog.github.io/HTML/images/avatar.png</logo>
    <icon>https://lizhenzhublog.github.io/HTML/favicon.ico</icon>
    <rights>All rights reserved 2020, Li Zhenzhu, Ph.D</rights>
    <entry>
        <title type="html"><![CDATA[elestrix]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/xnapG7-CO</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/xnapG7-CO">
        </link>
        <updated>2020-06-22T15:51:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="par0013">Par0013</h1>
<p>|</p>
<h2 id="内容">内容</h2>
<p>[ <a href="http://elastix.bigr.nl/wiki/index.php/Par0013">隐藏</a> ]</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Image_data">1 图像数据</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Application">2 申请</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Registration_settings">3 注册设置</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Published_in">4 发表于</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Other_comments">5 其他评论</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#References">6 参考</a></li>
</ul>
<p>|</p>
<h3 id="影像数据">影像数据</h3>
<ul>
<li>3D锥形束CT和2D X射线</li>
<li>神经介入性头/颈</li>
<li>X射线像素尺寸：0.86 x 0.86 x 1.0毫米; 尺寸：256 x 256 x 1</li>
</ul>
<p>（注意：2D图像应作为3D尺寸为1的3D图像给出）。</p>
<ul>
<li>CBCT体素尺寸：0.58 x 0.58 x 0.58 mm; 尺寸：256 x 256 x 256</li>
<li>使用X射线血管造影C型臂系统（Allura Xper FD20，Philips Healthcare，Best，荷兰）获得。</li>
<li>以MHD格式存储。</li>
<li>数据来源于马萨诸塞州伍斯特市麻省大学医学院放射学系。</li>
</ul>
<p>示例数据：</p>
<p><a href="http://elastix.bigr.nl/wiki/images/a/ae/ExampleData.zip" title="ExampleData.zip">媒体：ExampleData.zip</a></p>
<p>屏幕截图：</p>
<p><a href="http://elastix.bigr.nl/wiki/index.php/File:Screenshot_x-ray.jpg"><img src="http://elastix.bigr.nl/wiki/images/thumb/e/e3/Screenshot_x-ray.jpg/316px-Screenshot_x-ray.jpg" alt="截图x-ray.jpg"></a><a href="http://elastix.bigr.nl/wiki/index.php/File:Screenshot_x-ray_lat.jpg"><img src="http://elastix.bigr.nl/wiki/images/c/c3/Screenshot_x-ray_lat.jpg" alt="截图X射线lat.jpg"></a></p>
<h3 id="应用">应用</h3>
<p>比较了7种优化方法的组合的2D-3D注册性能：</p>
<p>*定期逐步下降
*内尔德米德
*鲍威尔·布伦特
*拟牛顿
*非线性共轭梯度
*同时摄动随机近似
*进化策略</p>
<p>和三个相似性度量：</p>
<p>*梯度差异
*归一化梯度相关
*图案强度。</p>
<p>相似性度量已添加到<tt>elastix中，</tt>并且仅<strong>用于2D-3D</strong>。</p>
<p>对在脑干预过程中获得的患者数据集进行了实验。评估了各种组分组合的配准精度，捕获范围和配准时间。结果表明，对于相同的相似性度量，使用不同的优化方法可获得不同的配准精度和捕获范围。总体而言，可以得出结论，就准确性，捕获范围和计算时间而言，Powell-Brent是将X射线图像基于强度的2D-3D注册到CBCT的可靠优化方法。</p>
<h3 id="注册设置">注册设置</h3>
<p><tt>elastix</tt>版本：4.5</p>
<p>使用单个X射线图像的参数文件：</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/images/3/3f/Par0013Powell_GD_singleImage.txt" title="Par0013鲍威尔GD singleImage.txt">媒体：par0013Powell_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/28/Par0013Powell_NGC_singleImage.txt" title="Par0013鲍威尔NGC singleImage.txt">媒体：par0013Powell_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/2c/Par0013Powell_PI_singleImage.txt" title="Par0013鲍威尔PI singleImage.txt">媒体：par0013Powell_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/0/04/Par0013NM_GD_singleImage.txt" title="Par0013NM GD singleImage.txt">媒体：par0013NM_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/19/Par0013NM_NGC_singleImage.txt" title="Par0013NM NGC singleImage.txt">媒体：par0013NM_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/7/76/Par0013NM_PI_singleImage.txt" title="Par0013NM PI singleImage.txt">媒体：par0013NM_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/9/94/Par0013GC_GD_singleImage.txt" title="Par0013GC GD singleImage.txt">媒体：par0013GC_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/16/Par0013GC_NGC_singleImage.txt" title="Par0013GC NGC singleImage.txt">媒体：par0013GC_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/f/f3/Par0013GC_PI_singleImage.txt" title="Par0013GC PI singleImage.txt">媒体：par0013GC_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/e/e4/Par0013QN_GD_singleImage.txt" title="Par0013QN GD singleImage.txt">媒体：par0013QN_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/6/60/Par0013QN_NGC_singleImage.txt" title="Par0013QN NGC singleImage.txt">媒体：par0013QN_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/4/4a/Par0013QN_PI_singleImage.txt" title="Par0013QN PI singleImage.txt">媒体：par0013QN_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/2d/Par0013RSGD_GD_singleImage.txt" title="Par0013RSGD GD singleImage.txt">媒体：par0013RSGD_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/6/69/Par0013RSGD_NGC_singleImage.txt" title="Par0013RSGD NGC singleImage.txt">媒体：par0013RSGD_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/0/0f/Par0013RSGD_PI_singleImage.txt" title="Par0013RSGD PI singleImage.txt">媒体：par0013RSGD_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/1d/Par0013CMAES_GD_singleImage.txt" title="Par0013CMAES GD singleImage.txt">媒体：par0013CMAES_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/3/36/Par0013CMAES_NGC_singleImage.txt" title="Par0013CMAES NGC singleImage.txt">媒体：par0013CMAES_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/e/e3/Par0013CMAES_PI_singleImage.txt" title="Par0013CMAES PI singleImage.txt">媒体：par0013CMAES_PI_singleImage.txt</a></li>
</ul>
<p>使用单个X射线图像的命令行调用：</p>
<p>elastix -f xrayImage.mhd -fMask xrayMask.mhd -m volumeData.mhd -p par0013Powel_NGC_singleImage.txt -out outputdir</p>
<p>使用多个X射线图像的参数文件（注意：示例显示2，但是可以接受更多固定图像）：</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/images/9/9d/Par0013Powell_NGC_twoImages.txt" title="Par0013鲍威尔NGC twoImages.txt">媒体：par0013Powell_NGC_twoImages.txt</a></li>
</ul>
<p>使用两个X射线图像进行命令行调用：</p>
<p>elastix -f0 xrayImage1.mhd -f1 xrayImage2.mhd -f0Mask xrayMask1.mhd -f1Mask xrayMask1.mhd -m volumeData.mhd -p par0013Powell_NGC_twoImages.txt -out outputdir</p>
<h3 id="出版于">出版于</h3>
<p>这些注册在出版物中描述：</p>
<p>IMJ van der Bom，S。Klein，M。Staring，R。Homan，LW Bartels，JPW Pluim，“在X射线引导的干预措施中基于强度的2D-3D配准的优化方法的评估”，载于：SPIE医学影像：图像处理，SPIE出版社，第一卷。7962，第796223-1-796223-15，2011年。</p>
<h3 id="其他的建议">其他的建议</h3>
<h3 id="参考资料">参考资料</h3>
<p>[1] IMJ van der Bom，S。Klein，M。Staring，R。Homan，LW Bartels，JPW Pluim，“在X射线引导的干预措施中基于强度的2D-3D配准的优化方法的评估”，载于：SPIE医学成像：图像处理，SPIE出版社，第1卷。7962，第796223-1-796223-15，2011年。</p>
<ul>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/Par0013" title="查看内容页面[alt-c]">页</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Talk:Par0013&amp;action=edit&amp;redlink=1" title="关于内容页面的讨论[alt-t]">讨论区</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;action=edit" title="此页面受保护。 您可以查看其来源[alt-e]">查看资料</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;action=history" title="此页面的先前修订版[alt-h]">历史</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Special:UserLogin&amp;returnto=Par0013" title="鼓励您登录； 但是，它不是强制性的[alt-o]">登录/创建账户</a></p>
</li>
</ul>
<h5 id="导航">导航</h5>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Main_Page" title="访问主页[alt-z]">主页</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Community_portal" title="关于项目，您可以做什么，在哪里找到东西">社区门户</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Current_events" title="查找有关当前事件的背景信息">现在发生的事</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:RecentChanges" title="Wiki [alt-r]中最近更改的列表">近期变动</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:Random" title="加载随机页面[alt-x]">随机页面</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Help:Contents" title="寻找的地方">救命</a></li>
</ul>
<h5 id="搜索">搜索</h5>
<h5 id="工具箱">工具箱</h5>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:WhatLinksHere/Par0013" title="在此处链接的所有维基页面的列表[alt-j]">这里有什么链接</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:RecentChangesLinked/Par0013" title="与此页面链接的页面的最新更改[alt-k]">相关变更</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:SpecialPages" title="所有特殊页面的列表[alt-q]">特殊页面</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;printable=yes">可打印的版本</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;oldid=981" title="永久链接到此页面的修订版">永久链接</a></li>
</ul>
<p><a href="http://www.gnu.org/licenses/old-licenses/fdl-1.2.txt"><img src="http://elastix.bigr.nl/wiki/skins/common/images/gnu-fdl.png" alt="GNU自由文档许可证1.2"></a></p>
<p><a href="http://www.mediawiki.org/"><img src="http://elastix.bigr.nl/wiki/skins/common/images/poweredby_mediawiki_88x31.png" alt="由MediaWiki提供支持"></a></p>
<ul>
<li>
<p>该页面的最后修改时间为2012年2月6日12:50。</p>
</li>
<li>
<p>本页面已经被浏览37,219次。</p>
</li>
<li>
<p>内容可在<a href="http://www.gnu.org/licenses/old-licenses/fdl-1.2.txt">GNU Free Documentation License 1.2下获得</a>。</p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Privacy_policy" title="elastix：隐私政策">隐私政策</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:About" title="elastix：关于">关于elastix</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:General_disclaimer" title="elastix：一般免责声明">免责声明</a></p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何利用AI创建一个空中橡皮擦]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/y0oFym3ys</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/y0oFym3ys">
        </link>
        <updated>2020-06-22T15:05:44.000Z</updated>
        <content type="html"><![CDATA[<p>如何利用AI创建一个空中橡皮擦
如果您可以在空中挥动笔以虚拟地绘制东西，然后实际上将其绘制在屏幕上，那会不会很酷？如果我们不使用任何特殊的硬件来实际实现这一目标，那可能会更加有趣，只需简单的计算机视觉即可，实际上，我们甚至不需要使用机器学习或深度学习来实现这一目标。</p>
<p>这是我们将构建的应用程序的演示。</p>
<p>因此，在本文中，您将学习如何创建自己的虚拟笔和虚拟橡皮擦。整个应用程序将从根本上构建轮廓检测。您可以将Contours看作是一条闭合的曲线，具有相同的颜色或强度，就像一个斑点，您可以<a href="https://docs.opencv.org/4.2.0/d4/d73/tutorial_py_contours_begin.html">在此处</a>阅读有关轮廓的更多信息  。</p>
<h2 id="它是如何工作的">它是如何工作的：</h2>
<p>因此，这是我们将如何实现此目的的方法，首先，我们将使用颜色遮罩来获得目标彩色笔的二进制遮罩（（<em>我将使用蓝色标记作为虚拟笔</em>）），然后使用轮廓检测​​来在整个屏幕上检测并跟踪该笔的位置。</p>
<p>一旦完成，只需按_字面_连接点  ，是的，您只需要使用笔的先前位置（<em>在前一帧中的位置</em>）的x，y坐标与新的x，y点（<em>x，y指向新框架</em>），就是这样，您有一支虚拟笔。</p>
<h2 id="结构体">结构体：</h2>
<p>当然，现在要进行预处理，并添加一些其他功能，因此这里是我们应用程序每个步骤的细分。</p>
<ol>
<li><strong>步骤1：</strong> <em>找到目标对象的颜色范围并保存。</em></li>
<li><strong>第2步：</strong> <em>应用正确的形态学操作以减少视频中的噪声</em></li>
<li><strong>步骤3：</strong> <em>通过轮廓检测来检测和跟踪有色物体。</em></li>
<li><strong>第4步：</strong> <em>找到要在屏幕上绘制的对象的x，y位置坐标。</em></li>
<li><em>**步骤5：**添加雨刮器功能以擦除整个屏幕。</em></li>
<li><em>**步骤6：**添加橡皮擦功能以擦除图形的某些部分。</em></li>
</ol>
<p>我已经设计了此应用程序的管道，以便可以轻松地将其重新用于其他项目，例如，如果要制作任何涉及跟踪彩色对象的项目，则可以使用步骤1-3。同样，这种故障使您自己运行一个步骤时调试起来变得容易得多，因为您将确切知道错误的步骤。每个步骤都可以独立运行。</p>
<p>请注意，我们在第4步已准备好虚拟笔，因此我在第5-6步中添加了更多功能，例如，在第5步中，有一个虚拟刮水器，可以像从屏幕上擦除笔一样从屏幕上擦除笔标记。然后在第6步中，我们将添加一个切换器，使您可以使用橡皮擦切换笔。因此，让我们开始吧。</p>
<p>首先导入所需的库</p>
<pre><code>import cv2
import numpy as np
import time

</code></pre>
<h3 id="步骤1找到目标笔的颜色范围并保存">步骤1：找到目标笔的颜色范围并保存</h3>
<p>首先，我们必须为目标彩色对象找到合适的颜色范围，该范围将在  <code>cv2.inrange()</code> 功能上用于过滤掉我们的对象。我们还将范围数组另存为<code>.npy</code>文件到磁盘中，以便以后访问。</p>
<p>由于我们正在尝试进行颜色检测，因此我们将RGB（或OpenCV中的BGR）格式的图像转换为HSV（色相，饱和度，值）颜色格式，因为它在该模型中更容易处理颜色。</p>
<p>下面的脚本将使您可以使用轨迹栏来调整图像的色相，饱和度和值通道。调整轨迹栏，直到只有您的目标对象可见，其余为黑色。</p>
<pre><code># A required callback method that goes into the trackbar function.
def nothing(x):
    pass

# Initializing the webcam feed.
cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# Create a window named trackbars.
cv2.namedWindow(&quot;Trackbars&quot;)

# Now create 6 trackbars that will control the lower and upper range of 
# H,S and V channels. The Arguments are like this: Name of trackbar, 
# window name, range,callback function. For Hue the range is 0-179 and
# for S,V its 0-255.
cv2.createTrackbar(&quot;L - H&quot;, &quot;Trackbars&quot;, 0, 179, nothing)
cv2.createTrackbar(&quot;L - S&quot;, &quot;Trackbars&quot;, 0, 255, nothing)
cv2.createTrackbar(&quot;L - V&quot;, &quot;Trackbars&quot;, 0, 255, nothing)
cv2.createTrackbar(&quot;U - H&quot;, &quot;Trackbars&quot;, 179, 179, nothing)
cv2.createTrackbar(&quot;U - S&quot;, &quot;Trackbars&quot;, 255, 255, nothing)
cv2.createTrackbar(&quot;U - V&quot;, &quot;Trackbars&quot;, 255, 255, nothing)
 
 
while True:
    
    # Start reading the webcam feed frame by frame.
    ret, frame = cap.read()
    if not ret:
        break
    # Flip the frame horizontally (Not required)
    frame = cv2.flip( frame, 1 ) 
    
    # Convert the BGR image to HSV image.
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Get the new values of the trackbar in real time as the user changes 
    # them
    l_h = cv2.getTrackbarPos(&quot;L - H&quot;, &quot;Trackbars&quot;)
    l_s = cv2.getTrackbarPos(&quot;L - S&quot;, &quot;Trackbars&quot;)
    l_v = cv2.getTrackbarPos(&quot;L - V&quot;, &quot;Trackbars&quot;)
    u_h = cv2.getTrackbarPos(&quot;U - H&quot;, &quot;Trackbars&quot;)
    u_s = cv2.getTrackbarPos(&quot;U - S&quot;, &quot;Trackbars&quot;)
    u_v = cv2.getTrackbarPos(&quot;U - V&quot;, &quot;Trackbars&quot;)
 
    # Set the lower and upper HSV range according to the value selected
    # by the trackbar
    lower_range = np.array([l_h, l_s, l_v])
    upper_range = np.array([u_h, u_s, u_v])
    
    # Filter the image and get the binary mask, where white represents 
    # your target color
    mask = cv2.inRange(hsv, lower_range, upper_range)
 
    # You can also visualize the real part of the target color (Optional)
    res = cv2.bitwise_and(frame, frame, mask=mask)
    
    # Converting the binary mask to 3 channel image, this is just so 
    # we can stack it with the others
    mask_3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
    
    # stack the mask, orginal frame and the filtered result
    stacked = np.hstack((mask_3,frame,res))
    
    # Show this stacked frame at 40% of the size.
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.4,fy=0.4))
    
    # If the user presses ESC then exit the program
    key = cv2.waitKey(1)
    if key == 27:
        break
    
    # If the user presses `s` then print this array.
    if key == ord('s'):
        
        thearray = [[l_h,l_s,l_v],[u_h, u_s, u_v]]
        print(thearray)
        
        # Also save this array as penval.npy
        np.save('penval',thearray)
        break
    
# Release the camera &amp; destroy the windows.    
cap.release()
cv2.destroyAllWindows()
</code></pre>
<h3 id="步骤2最大化检测掩模并消除噪声">步骤2：最大化检测掩模并消除噪声</h3>
<p>现在，不需要在上一步中获得完美的蒙版，也可以在图像中出现一些像白点这样的杂色，我们可以在此步骤中通过形态学操作消除这种杂色。</p>
<p>现在，我使用一个名为<strong>load_from_disk</strong>的变量   来确定是要从磁盘加载颜色范围还是要使用一些自定义值。</p>
<pre><code># This variable determines if we want to load color range from memory 
# or use the ones defined here. 
load_from_disk = True

# If true then load color range from memory
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# Creating A 5x5 kernel for morphological operations
kernel = np.ones((5,5),np.uint8)

while(1):
    
    ret, frame = cap.read()
    if not ret:
        break
        
    frame = cv2.flip( frame, 1 )

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise.
    # Erosion Eats away the white part while dilation expands it.
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)

    res = cv2.bitwise_and(frame,frame, mask= mask)

    mask_3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
    
    # stack all frames and show it
    stacked = np.hstack((mask_3,frame,res))
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.4,fy=0.4))
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤3追踪目标笔">步骤3：追踪目标笔</h3>
<p>现在我们有了一个不错的蒙版，我们可以使用它通过轮廓检测来检测笔。我们将在对象周围绘制一个边界框，以确保在整个屏幕上都可以检测到它。</p>
<pre><code># This variable determines if we want to load color range from memory 
# or use the ones defined in the notebook. 
load_from_disk = True

# If true then load color range from memory
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# kernel for morphological operations
kernel = np.ones((5,5),np.uint8)

# set the window to auto-size so we can view this full screen.
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This threshold is used to filter noise, the contour area must be 
# bigger than this to qualify as an actual contour.
noiseth = 500

while(1):
    
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower 
    # ranges from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours in the frame.
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL,
                                           cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also make sure its size 
    # is bigger than noise threshold.
    if contours and cv2.contourArea(max(contours, 
                               key = cv2.contourArea)) &gt; noiseth:
        
        # Grab the biggest contour with respect to area
        c = max(contours, key = cv2.contourArea)
        
        # Get bounding box coordinates around that contour
        x,y,w,h = cv2.boundingRect(c)
        
        # Draw that bounding box
        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,25,255),2)        

    cv2.imshow('image',frame)
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤4使用笔绘图">步骤4：使用笔绘图</h3>
<p>现在，所有内容都已设置好，我们可以轻松跟踪目标对象了，现在可以使用该对象在屏幕上进行虚拟绘制了。</p>
<p>现在我们只需要做的是使用  <code>x</code>，<code>y</code> 位置，距离返回  <strong><code>cv2.boundingRect()</code></strong> 功能从以前的帧（F-1），并将其与连接  <code>x</code>，<code>y</code> 物体的坐标，在新帧（F）。通过连接这两个点，我们绘制了一条线，并针对网络摄像头提要中的每一帧进行了绘制，通过这种方式，我们将看到使用笔进行的实时绘制。</p>
<p>**注意：**我们将在黑色画布上绘制，然后将该画布与框架合并。这是因为每次迭代都会获得一个新的框架，因此我们无法使用实际的框架。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

kernel = np.ones((5,5),np.uint8)

# Initializing the canvas on which we will draw upon
canvas = None

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initialize the canvas as a black image of the same size as the frame.
    if canvas is None:
        canvas = np.zeros_like(frame)

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also its size is bigger than 
    # the noise threshold.
    if contours and cv2.contourArea(max(contours, 
                                 key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        # This is true when we writing for the first time or when writing 
        # again when the pen had disappeared from view.
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            # Draw the line on the canvas
            canvas = cv2.line(canvas, (x1,y1),(x2,y2), [255,0,0], 4)
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
    # Merge the canvas and the frame.
    frame = cv2.add(frame,canvas)
    
    # Optionally stack both frames and show it.
    stacked = np.hstack((canvas,frame))
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.6,fy=0.6))

    k = cv2.waitKey(1) &amp; 0xFF
    if k == 27:
        break
        
    # When c is pressed clear the canvas
    if k == ord('c'):
        canvas = None

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤5添加图像刮水器">步骤5：添加图像刮水器</h3>
<p>在上面的脚本中，我们有一支可以正常工作的虚拟笔，并且当用户按下<code>c</code> 按钮时我们也清理或擦拭了屏幕  ，现在让我们也自动执行此擦拭部件的工作。一种简单的方法是检测目标对象何时离摄像机太近，然后如果目标离摄像机太近，则清除屏幕。</p>
<p>轮廓的大小随着它靠近相机而增加，因此我们可以监控轮廓的大小以实现此目的。</p>
<p>我们要做的另一件事是，我们还将警告用户我们将在几秒钟内清除屏幕，以便他/她可以将物体从框架中取出。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

kernel = np.ones((5,5),np.uint8)

# Making window size adjustable
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This is the canvas on which we will draw upon
canvas=None

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

# Threshold for wiper, the size of the contour must be bigger than for us to
# clear the canvas 
wiper_thresh = 40000

# A variable which tells when to clear canvas, if its True then we clear the canvas
clear = False

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initialize the canvas as a black image
    if canvas is None:
        canvas = np.zeros_like(frame)

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours.
    contours, hierarchy = cv2.findContours(mask,
    cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also its size is bigger than 
    # the noise threshold.
    if contours and cv2.contourArea(max(contours,
                                   key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # Get the area of the contour
        area = cv2.contourArea(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            # Draw the line on the canvas
            canvas = cv2.line(canvas, (x1,y1),(x2,y2),
            [255,0,0], 5)
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2
        
        # Now if the area is greater than the wiper threshold then set the  
        # clear variable to True and warn User.
        if area &gt; wiper_thresh:
           cv2.putText(canvas,'Clearing Canvas', (100,200), 
           cv2.FONT_HERSHEY_SIMPLEX,2, (0,0,255), 5, cv2.LINE_AA)
           clear = True 

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
   
    # Now this piece of code is just for smooth drawing. (Optional)
    _ , mask = cv2.threshold(cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY), 20, 
    255, cv2.THRESH_BINARY)
    foreground = cv2.bitwise_and(canvas, canvas, mask = mask)
    background = cv2.bitwise_and(frame, frame,
    mask = cv2.bitwise_not(mask))
    frame = cv2.add(foreground,background)

    cv2.imshow('image',frame)
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break
    
    # Clear the canvas after 1 second if the clear variable is true
    if clear == True:
        
        time.sleep(1)
        canvas = None
        
        # And then set clear to false
        clear = False
        
cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤6添加橡皮擦功能">步骤6：添加橡皮擦功能</h3>
<p>既然我们已经完成了笔和刮水器的工作，接下来就可以添加橡皮擦功能了。所以我想要的就是这样，当用户切换到橡皮擦而不是绘画时，它会擦除​​笔画的那一部分。做到这一点真的很容易，您只需要在画布上用黑色绘制橡皮擦就可以了。通过涂成黑色，该部分在合并期间恢复为原始状态，因此就像橡皮擦一样。橡皮擦功能的真正编码部分是如何在笔和橡皮擦之间进行切换，当然，最简单的方法是使用键盘按钮，但是我们想要比这更酷的东西。</p>
<p>因此，我们要做的就是每当有人将手放在屏幕的左上角时执行切换。我们将使用背景减法来监视该区域，以便我们知道何时会有干扰。就像按下虚拟按钮一样。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)

# Load these 2 images and resize them to the same size.
pen_img = cv2.resize(cv2.imread('pen.png',1), (50, 50))
eraser_img = cv2.resize(cv2.imread('eraser.jpg',1), (50, 50))

kernel = np.ones((5,5),np.uint8)

# Making window size adjustable
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This is the canvas on which we will draw upon
canvas = None

# Create a background subtractor Object
backgroundobject = cv2.createBackgroundSubtractorMOG2(detectShadows = False)

# This threshold determines the amount of disruption in the background.
background_threshold = 600

# A variable which tells you if you're using a pen or an eraser.
switch = 'Pen'

# With this variable we will monitor the time between previous switch.
last_switch = time.time()

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

# Threshold for wiper, the size of the contour must be bigger than this for # us to clear the canvas
wiper_thresh = 40000

# A variable which tells when to clear canvas
clear = False

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initilize the canvas as a black image
    if canvas is None:
        canvas = np.zeros_like(frame)
        
    # Take the top left of the frame and apply the background subtractor
    # there    
    top_left = frame[0: 50, 0: 50]
    fgmask = backgroundobject.apply(top_left)
    
    # Note the number of pixels that are white, this is the level of 
    # disruption.
    switch_thresh = np.sum(fgmask==255)
    
    # If the disruption is greater than background threshold and there has 
    # been some time after the previous switch then you. can change the 
    # object type.
    if switch_thresh&gt;background_threshold and (time.time()-last_switch) &gt; 1:

        # Save the time of the switch. 
        last_switch = time.time()
        
        if switch == 'Pen':
            switch = 'Eraser'
        else:
            switch = 'Pen'

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, 
    cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also it size is bigger than 
    # noise threshold.
    if contours and cv2.contourArea(max(contours,
                                      key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # Get the area of the contour
        area = cv2.contourArea(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            if switch == 'Pen':
                # Draw the line on the canvas
                canvas = cv2.line(canvas, (x1,y1),
                (x2,y2), [255,0,0], 5)
                
            else:
                cv2.circle(canvas, (x2, y2), 20,
                (0,0,0), -1)
            
            
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2
        
        # Now if the area is greater than the wiper threshold then set the 
        # clear variable to True
        if area &gt; wiper_thresh:
           cv2.putText(canvas,'Clearing Canvas',(0,200), 
           cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 1, cv2.LINE_AA)
           clear = True 

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
   
    # Now this piece of code is just for smooth drawing. (Optional)
    _ , mask = cv2.threshold(cv2.cvtColor (canvas, cv2.COLOR_BGR2GRAY), 20, 
    255, cv2.THRESH_BINARY)
    foreground = cv2.bitwise_and(canvas, canvas, mask = mask)
    background = cv2.bitwise_and(frame, frame,
    mask = cv2.bitwise_not(mask))
    frame = cv2.add(foreground,background)

    # Switch the images depending upon what we're using, pen or eraser.
    if switch != 'Pen':
        cv2.circle(frame, (x1, y1), 20, (255,255,255), -1)
        frame[0: 50, 0: 50] = eraser_img
    else:
        frame[0: 50, 0: 50] = pen_img

    cv2.imshow('image',frame)

    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break
    
    # Clear the canvas after 1 second, if the clear variable is true
    if clear == True: 
        time.sleep(1)
        canvas = None
        
        # And then set clear to false
        clear = False
        
cv2.destroyAllWindows()
cap.release()
</code></pre>
<p>注意：  我选择的所有这些不同阈值的值都将取决于您的环境，因此请先对其进行调整，而不要尝试使我的值起作用。</p>
<p>我用一张白纸盖住了蓝色标记，除了一张纸以外，所有面都用白纸盖住了，这样我就可以避免连续画图，并且在画图之间留有缝隙。
同样，对于需要通过颜色检测对象的任何应用程序，您可以重复使用步骤1-3。我希望你们中的一些人尝试扩展此应用程序，并可能构建更酷的功能。</p>
<p>希望您喜欢本教程，谢谢。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ITK教程 python]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/m-GVHor_Q</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/m-GVHor_Q">
        </link>
        <updated>2020-06-12T05:37:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="打开一个图片并且标准转化保存图片">打开一个图片并且标准转化保存图片</h1>
<pre><code>import itk
import sys
import vtk

inputfilename=&quot;D:/DATA/small/0200.dcm&quot;
outputfilename=&quot;D:/DATA/0200.png&quot;
#
# Reads a 2D image in with signed short (16bits/pixel) pixel type
# and save it as unsigned char (8bits/pixel) pixel type
#
InputImageType  = itk.Image.SS2
OutputImageType = itk.Image.UC2

reader = itk.ImageFileReader[InputImageType].New()
writer = itk.ImageFileWriter[OutputImageType].New()


filter = itk.RescaleIntensityImageFilter[InputImageType, OutputImageType].New()
filter.SetOutputMinimum( 0 )
filter.SetOutputMaximum(255)

filter.SetInput( reader.GetOutput() )
writer.SetInput( filter.GetOutput() )

reader.SetFileName(inputfilename )
writer.SetFileName(outputfilename )

writer.Update()
</code></pre>
<h1 id="itk打开后vtk查看">Itk打开后VTK查看</h1>
<p>在上述代码中加入</p>
<pre><code>ivfilter=itk.ImageToVTKImageFilter[OutputImageType].New()
ivfilter.SetInput(filter.GetOutput())
viewer=vtk.vtkImageViewer()
iren=vtk.vtkRenderWindowInteractor()
viewer.SetupInteractor(iren)
viewer.SetInput(ivfilter.GetOutput())
viewer.Render()
viewer.SetColorWindow(255)
viewer.SetColorLevel(128)
iren.Start()
</code></pre>
<h1 id="itk-配准的例子">ITK 配准的例子</h1>
<pre><code>from InsightToolkit import *
from sys import argv

fixedImageName=&quot;D:/DATA/Brain.png&quot;
movingImageName=&quot;D:/DATA/Brainshifted13x17y.png&quot;
outputImageName=&quot;D:/DATA/Brainresult2.png&quot;

fixedImageReader  = itkImageFileReaderIF2_New()
movingImageReader = itkImageFileReaderIF2_New()

fixedImageReader.SetFileName(  fixedImageName )
movingImageReader.SetFileName( movingImageName )

fixedImageReader.Update()
movingImageReader.Update()

fixedImage  = fixedImageReader.GetOutput()
movingImage = movingImageReader.GetOutput()

#
#  Instantiate the classes for the registration framework
#
registration = itkImageRegistrationMethodIF2IF2_New()
imageMetric  = itkMeanSquaresImageToImageMetricIF2IF2_New()
transform    = itkTranslationTransformD2_New()
optimizer    = itkRegularStepGradientDescentOptimizer_New()
interpolator = itkLinearInterpolateImageFunctionIF2D_New()


registration.SetOptimizer(    optimizer.GetPointer()    )
registration.SetTransform(    transform.GetPointer()    )
registration.SetInterpolator( interpolator.GetPointer() )
registration.SetMetric(       imageMetric.GetPointer()  )
registration.SetFixedImage(  fixedImage  )
registration.SetMovingImage( movingImage )

registration.SetFixedImageRegion( fixedImage.GetBufferedRegion() )

transform.SetIdentity()
initialParameters = transform.GetParameters()

registration.SetInitialTransformParameters( initialParameters )

#
# Iteration Observer
#
def iterationUpdate():
    currentParameter = transform.GetParameters()
    print &quot;M: %f   P: %f %f &quot; % ( optimizer.GetValue(),
                        currentParameter.GetElement(0),
                        currentParameter.GetElement(1) )

iterationCommand = itkPyCommand_New()
iterationCommand.SetCommandCallable( iterationUpdate )
optimizer.AddObserver( itkIterationEvent(), iterationCommand.GetPointer() )



#
#  Define optimizer parameters
#
optimizer.SetMaximumStepLength(  4.00 )
optimizer.SetMinimumStepLength(  0.01 )
optimizer.SetNumberOfIterations( 200  )


print &quot;Starting registration&quot;

#
#  Start the registration process
#

registration.Update()


#
# Get the final parameters of the transformation
#
finalParameters = registration.GetLastTransformParameters()

print &quot;Final Registration Parameters &quot;
print &quot;Translation X =  %f&quot; % (finalParameters.GetElement(0),)
print &quot;Translation Y =  %f&quot; % (finalParameters.GetElement(1),)

#
# Now, we use the final transform for resampling the
# moving image.
#
resampler = itkResampleImageFilterIF2IF2_New()
resampler.SetTransform( transform.GetPointer()    )
resampler.SetInput(     movingImage  )

region = fixedImage.GetLargestPossibleRegion()

resampler.SetSize( region.GetSize() )

resampler.SetOutputSpacing( fixedImage.GetSpacing() )
resampler.SetOutputOrigin(  fixedImage.GetOrigin()  )
resampler.SetOutputDirection(  fixedImage.GetDirection()  )
resampler.SetDefaultPixelValue( 100 )

outputCast = itkRescaleIntensityImageFilterIF2IUC2_New()
outputCast.SetOutputMinimum(      0  )
outputCast.SetOutputMaximum(  255  )
outputCast.SetInput(resampler.GetOutput())

#
#  Write the resampled image
#
writer = itkImageFileWriterIUC2_New()

writer.SetFileName( outputImageName )
writer.SetInput( outputCast.GetOutput() )
writer.Update()
print &quot;image registration has been finished&quot;
</code></pre>
<h2 id="usage">Usage</h2>
<h3 id="basic-examples">Basic examples</h3>
<p>Here is a simple python script that reads an image, applies a median image filter (radius of 2 pixels), and writes the resulting image in a file.</p>
<pre><code>#!/usr/bin/env python

import itk
import sys

input_filename = sys.argv[1]
output_filename = sys.argv[2]

image = itk.imread(input_filename)
median = itk.MedianImageFilter.New(image, Radius = 2)
itk.imwrite(median, output_filename)
</code></pre>
<p>There are two ways to instantiate filters with ITKPython:</p>
<ul>
<li>Implicit (recommended): ITK type information is automatically detected from the data. Typed filter objects and images are implicitly created.</li>
</ul>
<pre><code>
 # Use `ImageFileReader` instead of the wrapping function `imread` to illustrate this example.
reader = itk.ImageFileReader.New(FileName=input_filename)
# Here we specify the filter input explicitly
median = itk.MedianImageFilter.New(Input=reader.GetOutput())
# Same as above but shortened. `Input` does not have to be specified.
median = itk.MedianImageFilter.New(reader.GetOutput())
# Same as above. `.GetOutput()` does not have to be specified.
median = itk.MedianImageFilter.New(reader)

*   Explicit: This can be useful if a filter cannot automatically select the type information (e.g. &lt;cite&gt;CastImageFilter&lt;/cite&gt;), or to detect type mismatch errors which can lead to cryptic error messages.

Explicit instantiation of median image filter:

 # Use `ImageFileReader` instead of the wrapping function `imread` to illustrate this example.
reader = itk.ImageFileReader.New(FileName=input_filename)
# Here we specify the filter input explicitly
median = itk.MedianImageFilter.New(Input=reader.GetOutput())
# Same as above but shortened. `Input` does not have to be specified.
median = itk.MedianImageFilter.New(reader.GetOutput())
# Same as above. `.GetOutput()` does not have to be specified.
median = itk.MedianImageFilter.New(reader)

Explicit instantiation of cast image filter:

 image = itk.imread(input_filename)
InputType = type(image)
# Find input image dimension
input_dimension = image.GetImageDimension()
# Select float as output pixel type
OutputType = itk.Image[itk.UC, input_dimension]
castFilter = itk.CastImageFilter[InputType, OutputType].New()
castFilter.SetInput(image)
itk.imwrite(castFilter, output_filename)
</code></pre>
<h3 id="itk-python-types">ITK Python types</h3>
<colgroup><col width="51%"><col width="49%"></colgroup>
| C++ type | Python type |
| --- | --- |
| float | itk.F |
| double | itk.D |
| unsigned char | itk.UC |
| std::complex<float> | itk.complex[itk.F] |
<p>This list is not exhaustive and is only presented to illustrate the type names. The complete list of types can be found in the <a href="https://itk.org/ITKSoftwareGuide/html/Book1/ITKSoftwareGuide-Book1ch9.html#x48-1530009.5">ITK Software Guide</a>.</p>
<p>Types can also be obtained from their name in the C programming language:</p>
<p>itk.F == itk.ctype('float') # True</p>
<h3 id="instantiate-an-itk-object">Instantiate an ITK object</h3>
<p>There are two types of ITK objects. Most ITK objects (images, filters, adapters, …) are instantiated the following way:</p>
<p>InputType = itk.Image[itk.F,3]
OutputType = itk.Image[itk.F,3]
median = itk.MedianImageFilter[InputType, OutputType].New()</p>
<p>Some objects (matrix, vector, RGBPixel, …) do not require the attribute <cite>.New()</cite> to be added to instantiate them:</p>
<p>pixel = itk.RGBPixel<a href="">itk.D</a></p>
<p>In case of doubt, look at the attributes of the object you are trying to instantiate.</p>
<h2 id="mixing-itk-and-numpy">Mixing ITK and NumPy</h2>
<p>A common use case for using ITK in Python is to mingle NumPy and ITK operations on raster data. ITK provides a large number of I/O image formats and several sophisticated image processing algorithms not available in any other packages. The ability to intersperse that with numpy special purpose hacking provides a great tool for rapid prototyping.</p>
<p>The following script shows how to integrate NumPy and ITK:</p>
<pre><code>
 import itk
import numpy as np

# Read input image
itk_image = itk.imread(input_filename)

# Run filters on itk.Image

# View only of itk.Image, data is not copied
np_view = itk.GetArrayViewFromImage(itk_image)

# Copy of itk.Image, data is copied
np_copy = itk.GetArrayFromImage(itk_image)

# Do numpy stuff

# Convert back to itk, view only, data is not copied
itk_np_view = itk.GetImageViewFromArray(np_copy)

# Convert back to itk, data is copied
itk_np_copy = itk.GetImageFromArray(np_copy)

# Save result
itk.imwrite(itk_np_view, output_filename)
</code></pre>
<p>Similar functions are available to work with VNL vector and matrices:</p>
<pre><code> # Vnl matrix from array
arr = np.zeros([3,3], np.uint8)
matrix = itk.GetVnlMatrixFromArray(arr)

# Array from Vnl matrix
arr = itk.GetArrayFromVnlMatrix(matrix)

# Vnl vector from array
vec = np.zeros([3], np.uint8)
vnl_vector = itk.GetVnlVectorFromArray(vec)

# Array from Vnl vector
vec = itk.GetArrayFromVnlVector(vnl_vector)
</code></pre>
<h2 id="examples">Examples</h2>
<p>Examples can be found in the <a href="https://itk.org/ITKExamples/src/index.html">ITKExamples project</a>.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[slicer 中删除Node]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/4xnWUQLST</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/4xnWUQLST">
        </link>
        <updated>2020-06-12T05:22:19.000Z</updated>
        <content type="html"><![CDATA[<p>如何利用slicer中删除相关的数据</p>
<pre><code>RemoveNodeFromScene(shNode)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何在slicer中沿着一条弧线移动]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y">
        </link>
        <updated>2020-06-11T03:17:34.000Z</updated>
        <content type="html"><![CDATA[<p>如何在slicer中沿着一条弧线移动</p>
<pre><code># Load sample volume
import SampleData
sampleDataLogic = SampleData.SampleDataLogic()
mrHead = sampleDataLogic.downloadMRHead()

# Create transform and apply to sample volume
transformNode = slicer.vtkMRMLTransformNode()
slicer.mrmlScene.AddNode(transformNode)
mrHead.SetAndObserveTransformNodeID(transformNode.GetID())

# How to move a volume along a trajectory using a transform:
import time
import math
transformMatrix = vtk.vtkMatrix4x4()
for xPos in range(-30,30):
  transformMatrix.SetElement(0,3, xPos)
  transformMatrix.SetElement(1,3, math.sin(xPos)*10)
  transformNode.SetMatrixTransformToParent(transformMatrix)
  slicer.app.processEvents()
  time.sleep(0.02)
# Note: for longer animations use qt.QTimer.singleShot(100, callbackFunction)
# instead of a for loop.
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[更改点的颜色]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0">
        </link>
        <updated>2020-06-11T03:05:28.000Z</updated>
        <content type="html"><![CDATA[<p>更改点的颜色要注意了
错误的代码：</p>
<pre><code>markupNode = getNode('MarkupsCurve')
displayNode = markupNode.GetDisplayNode()
displayNode.SetColor([0,0,0])
</code></pre>
<p>注意：Markups have Color and SelectedColor properties. SelectedColor is used if all control points are in “selected” state, which is the default. So, in most cases you want to use SetSelectedColor.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Slicer的VMTK扩展]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/T7OLytblD</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/T7OLytblD">
        </link>
        <updated>2020-06-11T02:58:48.000Z</updated>
        <summary type="html"><![CDATA[<p>此扩展使3D Slicer（www.slicer.org）中提供了血管建模工具包（VMTK，http：//www.vmtk.org/）。功能包括血管树分割和中心线提取。此处提供了有关血管分割和中心线提取的简短演示视频：https : //youtu.be/caEuwJ7pCWs</p>
]]></summary>
        <content type="html"><![CDATA[<p>此扩展使3D Slicer（www.slicer.org）中提供了血管建模工具包（VMTK，http：//www.vmtk.org/）。功能包括血管树分割和中心线提取。此处提供了有关血管分割和中心线提取的简短演示视频：https : //youtu.be/caEuwJ7pCWs</p>
<!-- more --> 
<h1 id="安装">安装</h1>
<p>VMTK扩展可用于最新的3D Slicer版本（Slicer-4.10及更高版本）。安装3D Slicer，启动3D Slicer，然后在扩展管理器中安装SlicerVMTK扩展。</p>
<h1 id="用法">用法</h1>
<p>SlicerVMTK扩展提供以下模块-在模块列表的“血管建模工具包”类别中列出。</p>
<h2 id="血管过滤">血管过滤</h2>
<p>图像处理操作可增加管状结构的亮度并抑制其他形状（板和斑点）。该模块可用于预处理图像数据，以使血管分割更容易。</p>
<h2 id="水平集细分">水平集细分</h2>
<p>该模块可以根据图像分割血管树的单个血管分支（可以使用未处理或经过血管过滤的血管）。</p>
<h2 id="中心线计算">中心线计算</h2>
<p>从输入模型节点确定容器树中的中心线。单击“预览”按钮以快速验证输入模型和近似中心线计算。单击“开始”按钮进行完整的网络分析和所有输出的计算。</p>
<p>所需输入：</p>
<p>血管树模型：可以是任何树结构（不仅是血管树，还可以是气道等），可以在“段编辑器”模块中创建，也可以使用“级别集分割”模块创建。如果使用了细分编辑器，则必须通过在“数据”模块中的细分上单击鼠标右键，然后选择“将可见细分导出到模型”，将细分节点导出到模型节点。
起点：包含单个点的标记基准节点，应将其放置在树的分支处
输出：</p>
<p>中心线模型：网络提取结果（无分支提取和合并）。点包含中心线点，“半径”点数据包含每个点的最大内切球半径。
中心线端点：找到的起点和所有检测到的分支端点的坐标。
Voronoi模型（可选）：输入模型的中间表面（中间表面包含的点与最近的表面点的距离等于q）
曲线树角色（可选）：如果选择了标记曲线节点，则将从提取和合并的分支中创建曲线节点的层次结构。每个分支的CellId将保存到节点的名称中（也保存为节点属性），该名称可用于与中心线属性表的CellId列进行交叉引用。
中心线属性（可选）：如果选择了表节点，则将为所有提取和合并的分支计算分支长度，平均半径，曲率，扭转和曲折度。</p>
<h1 id="高级功能">高级功能：</h1>
<h2 id="提取中心线的点坐标和半径">提取中心线的点坐标和半径</h2>
<pre><code>c = getNode('CenterlineComputationModel')
points = slicer.util.arrayFromModelPoints(c)
radii = slicer.util.arrayFromModelPointData(c, 'Radius')
for i, radius in enumerate(radii):
  print(&quot;Point {0}: position={1}, radius={2}&quot;.format(i, points[i], radius))
</code></pre>
<h2 id="把中心线的点和线转换为三维的vtk">把中心线的点和线转换为三维的vtk</h2>
<pre><code>centerlineModel = getNode('CenterlineComputationModel')
centerlinePoly = centerlineModel.GetPolyData()

# Get first point position:
print(centerlinePoly.GetPoints().GetPoint(0))

# Get point IDs of the first line segment
pointIds = vtk.vtkIdList()
centerlinePoly.GetLines().GetCell(0, pointIds)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D slicer 提取STL中心线]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/qw9_t-Cwj</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/qw9_t-Cwj">
        </link>
        <updated>2020-06-11T02:52:44.000Z</updated>
        <content type="html"><![CDATA[<p>可以利用VMTK扩展</p>
<pre><code>reader = vmtkscripts.vmtkSurfaceReader()
reader.InputFileName = ‘input.stl’
reader.Execute()

# network extraction
networkExtraction = vmtkscripts.vmtkNetworkExtraction()
networkExtraction.Surface = reader.Surface
networkExtraction.Execute()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python+OpenCV录制双目摄像头视频]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/dL-YnYF3i</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/dL-YnYF3i">
        </link>
        <updated>2020-04-22T12:52:47.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>第一个例子
说起来录制视频，我们可能有很多的软件，但是比较坑的是，好像很少的软件支持能够同时录制两个摄像头的视频，于是我们用python自己写一个。要是OpenCV+python
https://github.com/anonymouslycn/bjtu_BinocularCameraRecord</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>第一个例子
说起来录制视频，我们可能有很多的软件，但是比较坑的是，好像很少的软件支持能够同时录制两个摄像头的视频，于是我们用python自己写一个。要是OpenCV+python
https://github.com/anonymouslycn/bjtu_BinocularCameraRecord</li>
</ul>
<!-- more -->
<pre><code>import cv2
import numpy as np
from PyQt5.QtWidgets import (QMainWindow, QApplication, QFileDialog)
import threading
import threadpool 
from CvPyGui import ImageCvQtContainer
from CvPyGui.ui import gui


#UI

class Image(QWidget):
 &quot;&quot;&quot;Common base for the images&quot;&quot;&quot;
 
 def __init__(self, name, label):
 super().__init__()
 
 # Label (frame) where the original image will be located, with scaling
 self.frame_lbl = label
 
 def updateImage(self, opencv_rgb_image):
 
 self.cv_img_rgb = opencv_rgb_image
 
 height, width, channel = self.cv_img_rgb.shape
 bytesPerLine = 3 * width
 self.q_image = QImage(self.cv_img_rgb.data, width,
  height, bytesPerLine, QImage.Format_RGB888)
 
 self.frame_lbl.setPixmap(QPixmap.fromImage(self.q_image))
 
 def saveImage(self):
 # Function for saving the processed image
 
 filter = &quot;Images (*.png *.jpg)&quot;
 
 image_path, _ = QFileDialog.getSaveFileName(self, filter=filter)
 
 cv_img_bgr = cv2.cvtColor(
 self.cv_img_rgb, cv2.COLOR_RGB2BGR)
 cv2.imwrite(image_path, cv_img_bgr)
 
 #获取视频
cap = cv2.VideoCapture(int(text))
cap.set(6 ,cv2.VideoWriter_fourcc('M', 'J', 'P', 'G') );
cap.set(3,w);
cap.set(4,h);
global update1
update1 = 1
global shotmark1
ret, frame = cap.read()


</code></pre>
<p>这样就能够获取到一帧图像了，其中cap.set()函数用来设置相机的参数，本来应该有宏定义的，但是在python里面老是报错，直接用数字替代了，其中3就是获取视频的宽度像素，4是高度，这个要和摄像头手册上的参数一致。一般的Webcam有两种图像获取格式：一种是YUV2格式这种事10bit回传的数据，理论上质量更好，但是有个很大的问题是分辨率高的时候，帧率就会变得十分低。另一种格式是MJPEG格式，这个是使用了压缩技术得到的视频流。通过这个格式，手册上说在1920x1080分辨率下都能获得30fps的表现，而YUV2只有5fps（后来发现，这个就是坑爹的，信了就怪了）。cap.set(6 ,cv2.VideoWriter_fourcc(‘M', ‘J', ‘P', ‘G') );这个参数就是使用MJPEG格式来读取摄像头的数据。</p>
<p>多线程</p>
<p>刚才我们呢也提到了，cap.read()这个函数是获取到了一帧图像，但是呢。我们要的是动画啊，要是写个循环的话，又会吧进程卡死在循环中，照成假死的状态，所以对于图像的绘制，一定要使用多线程技术。在这里我不仅要吐槽一下了。学了好多年计算机，讲了很多串行算法和编程，一讲到多线程，无非就是打印个Hello World！，根本就没有什么实践，理论倒是学了很多，感觉用的时候头真的好大！</p>
<p>其实这里的多线程也没有什么是吧，就是起调一下。但是要注意的是要控制线程的退出，在python这个我引入的多线程包里面，贼坑的是没有外界控制线程退出的办法！所以，我设置了一个全局变量，使用判断全局变量的值来判断是否让子线程继续下去。</p>
<p>结尾</p>
<p>实际上，还有分辨率/帧率设置功能呢，只不过懒得写了！！！</p>
<ul>
<li>另一例子</li>
</ul>
<pre><code>import cv2

cap = cv2.VideoCapture(0) #默认大小
# cap.set(3,1080)
# cap.set(4,720)
cv2.namedWindow(cv2.WINDOW_NORMAL)#任意调节大小

#显示
import cv2

# 初始化摄像头
cap = cv2.VideoCapture(0)

while cap.isOpened():
    # 采集一帧一帧的图像数据
    isSuccess,frame = cap.read()
    # 实时的将采集到的数据显示到界面上
    if isSuccess:
        cv2.imshow(&quot;My Capture&quot;,frame)
    # 实现按下“q”键退出程序
    if cv2.waitKey(1)&amp;0xFF == ord('q'):
        break

# 释放摄像头资源
cap.release()
cv2.destoryAllWindows()
</code></pre>
<p>录制视频</p>
<pre><code># coding:utf-8
import sys

reload(sys)
sys.setdefaultencoding('utf8')
#    __author__ = '郭 璞'
#    __date__ = '2016/9/7'
#    __Desc__ = 使用Python借助opencv实现对图像的读取，写入

import cv2
import numpy as np
# 选取摄像头，0为笔记本内置的摄像头，1,2···为外接的摄像头
cap = cv2.VideoCapture(0)
# cap.set(3,1080)
# cap.set(4,720)

# 为保存视频做准备
fourcc = cv2.cv.CV_FOURCC(&quot;D&quot;, &quot;I&quot;, &quot;B&quot;, &quot; &quot;)
# 第三个参数则是镜头快慢的，20为正常，小于二十为慢镜头
out = cv2.VideoWriter('output2.avi', fourcc,3.0,(640,480))
while True:
    # 一帧一帧的获取图像
    ret,frame = cap.read()
    if ret == True:
        frame = cv2.flip(frame, 1)
        # 在帧上进行操作
        # gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
        # 开始保存视频
        out.write(frame)
        # 显示结果帧
        cv2.imshow(&quot;frame&quot;, frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
    else:
        break
# 释放摄像头资源
cap.release()
out.release()
cv2.destroyAllWindows()
</code></pre>
<ul>
<li>另一个例子，我电脑上可以实现的</li>
</ul>
<pre><code>import numpy as np
import cv2

cap = cv2.VideoCapture(0)

fourcc = cv2.VideoWriter_fourcc(*'XVID')
#out = cv2.VideoWriter('output.mp4',fourcc, 30.0, (640,480))  #设置分辨率


# 获取捕获的分辨率
# propId可以直接写数字，也可以用OpenCV的符号表示
width, height = cap.get(3), cap.get(4)
print(width, height)
out = cv2.VideoWriter('output.mp4',fourcc, 30.0, (int(width),int(height)))
'''
# 以原分辨率的一倍来捕获
cap.set(cv2.CAP_PROP_FRAME_WIDTH, width * 2)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height * 2)
'''


while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        frame = cv2.flip(frame,1)
#        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))   将视频转换为灰色的源
        out.write(frame)

        cv2.imshow('frame', frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('1'):
            break
    else:
        break

cap.release()
out.release()
cv2.destroyAllWindows()

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何将视频转换为图片]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/dx7xPaKP-</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/dx7xPaKP-">
        </link>
        <updated>2020-04-22T11:30:05.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>如何将视频转换为图片</li>
</ul>
<p>更具根据就</p>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>如何将视频转换为图片</li>
</ul>
<p>更具根据就</p>
<!-- more -->
<pre><code>import numpy as np
import cv2, PIL, os
from cv2 import aruco
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
%matplotlib nbagg

workdir = &quot;../Aruco/data/calib_tel_ludo2/&quot;
name = &quot;VID_20180406_104312.mp4&quot;
rootname = name.split(&quot;.&quot;)[0]
cap = cv2.VideoCapture(workdir + name)
counter = 0
each = 10
length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
for i in range(length):
    ret, frame = cap.read()
    if i % each == 0: cv2.imwrite(workdir + rootname + &quot;_{0}&quot;.format(i) + &quot;.png&quot;, frame)

cap.release()

os.listdir(&quot;../Aruco/data/calib_tel_ludo/&quot;)
int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
</code></pre>
]]></content>
    </entry>
</feed>