<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lizhenzhublog.github.io/HTML</id>
    <title>Li Zhenzhu, Ph.D</title>
    <updated>2020-06-27T08:28:12.975Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lizhenzhublog.github.io/HTML"/>
    <link rel="self" href="https://lizhenzhublog.github.io/HTML/atom.xml"/>
    <subtitle>Binzhou Medical University Hospital, Email: timeanddoctor@gmail.com.</subtitle>
    <logo>https://lizhenzhublog.github.io/HTML/images/avatar.png</logo>
    <icon>https://lizhenzhublog.github.io/HTML/favicon.ico</icon>
    <rights>All rights reserved 2020, Li Zhenzhu, Ph.D</rights>
    <entry>
        <title type="html"><![CDATA[3D slicer后台加载module]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/47tOmy018</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/47tOmy018">
        </link>
        <updated>2020-06-27T08:27:32.000Z</updated>
        <content type="html"><![CDATA[<pre><code>import ExtensionWizard as ew
path='D:/DICOM2PNG'
ew.ExtensionWizardWidget().loadModules(path=path)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D slicer中视频的分辨率]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/I9fNHn4wV</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/I9fNHn4wV">
        </link>
        <updated>2020-06-27T05:01:18.000Z</updated>
        <content type="html"><![CDATA[<p>默认情况下，在3D视图中显示的图像分辨率与在切片视图中显示的图像分辨率相同。在极端情况下，这确实可能会导致在3D视图中显示较低的分辨率，这可能不是您所期望的：</p>
<p><a href="https://aws1.discourse-cdn.com/standard17/uploads/slicer/original/2X/5/58d094cc8d9ac601cde14800feef9bc8bf35c7ec.jpeg" title="图片"><img src="https://aws1.discourse-cdn.com/standard17/uploads/slicer/optimized/2X/5/58d094cc8d9ac601cde14800feef9bc8bf35c7ec_2_690x497.jpeg" alt="图片"></a>
图片1640×1183 547 KB</p>
<p>如果要将3D视图的分辨率与切片视图的分辨率解耦，则可以在切片视图可见性子菜单中选择“间距匹配体积”选项之一：</p>
<p><a href="https://aws1.discourse-cdn.com/standard17/uploads/slicer/original/2X/3/31c48789697f356cfd0506090a9aebb30daed128.jpeg" title="图片"><img src="https://aws1.discourse-cdn.com/standard17/uploads/slicer/optimized/2X/3/31c48789697f356cfd0506090a9aebb30daed128_2_690x487.jpeg" alt="图片"></a>
图片1676×1184 835 KB</p>
<p>您使用哪种图像</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[slicer 中 批量使用注册]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/mhUo-OVLB</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/mhUo-OVLB">
        </link>
        <updated>2020-06-25T05:26:14.000Z</updated>
        <content type="html"><![CDATA[<h1 id="核心代码">**核心代码：</h1>
<pre><code>elastixLogic.registerVolumes(
            fixedVolume, movingVolume,
            outputVolumeNode = outputVol,
            parameterFilenames = parameterFilenames,
            outputTransformNode = outputTransform
            )
</code></pre>
<h1 id="批量注册的代码">批量注册的代码****</h1>
<pre><code>## Run the following command in Python Console.
# execfile(&quot;path\to\BatchRegister.py&quot;)
def Register(Fixedfilename,Movingfilename,OutVolumefilename,ind):
    Nodename = 'Volume_{:02d}'.format(ind)
    RegistrationPresets_ParameterFilenames = 5
    # Load Volume
    [ success,movingVolumeNode ] = slicer.util.loadVolume(Movingfilename,returnNode=True)
    [ success, fixedVolumeNode] = slicer.util.loadVolume(Fixedfilename,returnNode=True)
    

    from Elastix import ElastixLogic
    logic = ElastixLogic()
    parameterFilenames = logic.getRegistrationPresets()[0][RegistrationPresets_ParameterFilenames]
    outputVolume = slicer.vtkMRMLScalarVolumeNode()
    slicer.mrmlScene.AddNode(outputVolume)
    outputVolume.CreateDefaultDisplayNodes()
    outputVolume.SetName(Nodename)
    logic.registerVolumes(fixedVolumeNode, movingVolumeNode, parameterFilenames = parameterFilenames , outputVolumeNode = outputVolume)

    # Create OutputVolume Node.
    myNode = getNode(Nodename)
    myStorageNode = myNode.CreateDefaultStorageNode()
    myStorageNode.SetFileName(OutVolumefilename)
    myStorageNode.WriteData(myNode)
    slicer.mrmlScene.Clear(0)



def BatchRegister():
    path2subjects = 'path\to\dataset_directory'
    import os
    for ind,dir in enumerate(os.listdir(path2subjects)):
        Fixedfilename = os.path.join(path2subjects,dir,'Fixed.ext')
        print(Fixedfilename)
        Movingfilename = os.path.join(path2subjects,dir,'Moving.ext')
        OutVolumefilename = os.path.join(path2subjects,dir,'OutVolume.ext')
        Register(Fixedfilename, Movingfilename, OutVolumefilename,ind)
BatchRegister()
</code></pre>
<h1 id="运行程序">运行程序：</h1>
<pre><code>2.Execute the Python script in the Slicer’s Python command as follow:
&gt;&gt;execfile(&quot;path\to\BatchRegister.py&quot;)
</code></pre>
<h2 id="第二种方法">第二种方法</h2>
<pre><code>Slicer.exe path\to\BatchRegister.py
</code></pre>
<h1 id="很好的思维利用try">很好的思维，利用try</h1>
<pre><code> try:
            import Elastix
            elastixLogic = Elastix.ElastixLogic()
            parameterFilenames                =elastixLogic.getRegistrationPresets([0]Elastix.RegistrationPresets_ParameterFilenames]
            useelastix = True
 except Exception as e:
            print(e)
            useelastix = False
						
	elastixLogic.registerVolumes(
                                densRnode['node'], densnode['node'],
                                parameterFilenames = parameterFilenames,
                                outputTransformNode = transformNode
                                )
</code></pre>
<h1 id="另一个比较直接的">另一个比较直接的</h1>
<pre><code>import Elastix
FixedVolume= slicer.util.loadVolume(‘L:/briend/Olivier_Slicer_try1/avg152T1.nii’, returnNode=True)
n=slicer.util.getNode(‘avg15*’)
MovingVolume= slicer.util.loadVolume(‘L:/briend/Olivier_Slicer_try1/t0697_t1_s03.nii’, returnNode=True)
m=slicer.util.getNode(‘t06*’)
outputVolume = slicer.vtkMRMLScalarVolumeNode()
slicer.mrmlScene.AddNode(outputVolume)
outputVolume.CreateDefaultDisplayNodes()
logic = Elastix.ElastixLogic()
RegistrationPresets_ParameterFilenames = 5
parameterFilenames = logic.getRegistrationPresets()[0][RegistrationPresets_ParameterFilenames]
logic.registerVolumes(n, m, parameterFilenames = parameterFilenames, outputVolumeNode = outputVolume)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[使用已知组件的3D–2D图像配准进行机器人钻机引导定位]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/POl9dIzPa</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/POl9dIzPa">
        </link>
        <updated>2020-06-23T14:02:13.000Z</updated>
        <content type="html"><![CDATA[<p>翻译免责声明</p>
<p>2018年2月6日使用已知组件的3D–2D图像配准进行机器人钻机引导定位</p>
<p><a href="https://www.spiedigitallibrary.org/profile/notfound?author=Thomas_Yi">托马斯毅</a>， <a href="https://www.spiedigitallibrary.org/profile/notfound?author=Vignesh_Ramchandran">维涅什Ramchandran的</a>， <a href="https://www.spiedigitallibrary.org/profile/Jeffrey.Siewerdsen-14695">杰弗里·H. Siewerdsen</a>， <a href="https://www.spiedigitallibrary.org/profile/Ali.Uneri-131532">阿里Uneri</a></p>
<p><a href="https://www.spiedigitallibrary.org/">所属单位+</a></p>
<p><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2">医学成像杂志，5（2）</a>，021212（2018）。<a href="https://doi.org/10.1117/1.JMI.5.2.021212">https://doi.org/10.1117/1.JMI.5.2.021212</a></p>
<p>抽象</p>
<p>X射线图像引导的机器人器械定位的方法已被报道并在脊椎椎弓根螺钉置入的临床前研究中进行了评估，目的是改善经椎弓根K线和螺钉的输送。已知组件（KC）配准算法用于将三维患者CT和钻探引导表面模型配准到术中二维X线照片。进行的转换与离线手眼校准相结合，可驱动机器人握住的钻具导向器，使其到达术前CT中定义的目标轨迹。与更传统的基于跟踪器的方法相比，对该方法进行了评估，并在幻影和尸体中测试了对临床实际错误的鲁棒性。根据工具提示处的目标配准误差（TRE）（mm）和接近角（deg）分析了与计划轨迹的偏差。在幻像研究中，KC方法产生的TRE = 1.51±0.51 mm和1.01度±0.92度，与基于跟踪器的方法的准确性相当。在具有实际解剖变形的尸体研究中，KC方法产生的TRE = 2.31±1.05 mm和0.66 deg±0.62 deg，与追踪器相比有统计学上的显着改善（TRE = 6.09±1.22 mm和1.06 deg±0.90 deg）。变形的鲁棒性归因于射线照相图中解剖结构的相对局部刚度。X射线引导可提供准确的机器人定位，并且可以自然地适合于在透视引导下的临床工作流程。与基于跟踪器的方法的准确性相当。在具有实际解剖变形的尸体研究中，KC方法产生的TRE = 2.31±1.05 mm和0.66 deg±0.62 deg，与追踪器相比有统计学上的显着改善（TRE = 6.09±1.22 mm和1.06 deg±0.90 deg）。变形的鲁棒性归因于射线照相图中解剖结构的相对局部刚度。X射线引导可提供准确的机器人定位，并且可以自然地适合于在透视引导下的临床工作流程。与基于跟踪器的方法的准确性相当。在具有实际解剖变形的尸体研究中，KC方法产生的TRE = 2.31±1.05 mm和0.66 deg±0.62 deg，与追踪器相比有统计学上的显着改善（TRE = 6.09±1.22 mm和1.06 deg±0.90 deg）。变形的鲁棒性归因于射线照相图中解剖结构的相对局部刚度。X射线引导可提供准确的机器人定位，并且可以自然地适合于在透视引导下的临床工作流程。变形的鲁棒性归因于射线照相图中解剖结构的相对局部刚度。X射线引导可提供准确的机器人定位，并且可以自然地适合于在透视引导下的临床工作流程。变形的鲁棒性归因于射线照相图中解剖结构的相对局部刚度。X射线引导可提供准确的机器人定位，并且可以自然地适合于在透视引导下的临床工作流程。</p>
<p><a id="sec1" data-feathr-click-track="true"></a></p>
<h2 id="1">1。</h2>
<h2 id="介绍">介绍</h2>
<p>在脊柱内固定中，准确地将螺钉放置在椎骨内是至关重要的，这是脊柱外科手术越来越普遍的一种，据报道在过去十年中增长了70％。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r1"><sup>1</sup></a>椎弓根螺钉通过对骨骼提供牢固的锚固，为各种外科手术构造物奠定了稳定的基础。螺钉放置涉及针对关键结构附近的椎体（椎弓根）内狭窄的骨通道，对多个椎骨水平重复此操作。将螺钉完全固定在椎弓根内即可成功放置。据报道，通常由骨解剖结构外的骨折引起的螺钉移位不良在腰椎中占5％至41％，在胸椎中占3％至55％。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r2"><sup>2</sup></a>尽管一小部分此类错位与明显的合并症相关，但由于神经血管损伤的可能性<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r3"><sup>3</sup></a>和昂贵的翻修手术，已探索了<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r4"><sup>4种</sup></a>多种方法来提高植入的准确性。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r5"><sup>5</sup></a><sup>，</sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r6"><sup>6</sup></a></p>
<p>手动放置椎弓根螺钉通常需要进行X射线透视检查，以识别解剖特征并从二维（2D）投影视图评估螺钉的位置和深度。最近，基于CT的导航（使用术前CT，术中CT或术中锥形束CT）可以提高椎弓根螺钉放置的精确度。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r6"><sup>6</sup></a>特别是，荟萃分析报告显示，完全放置在椎弓根中的导航辅助放置螺钉的比率更高（平均95.2％），而没有导航的放置螺钉的比率更高（平均90.3％）。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r7"><sup>7</sup></a>相反，尽管用于导航的外科手术跟踪技术（例如在颅神经外科中）已得到很好的确立，但在使用荧光检查的过程中（例如在脊柱神经外科或骨盆创伤中），其采用率较低，部分原因是与现有的临床工作流程相比，其复杂性，对附加设备的要求，视线以及由解剖变形引起的潜在误差。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r8"><sup>8</sup></a><sup>，</sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r9"><sup>9</sup></a>这样的限制激励的，可以甚至在没有附加的跟踪系统一起使用，以提高性能，使更广泛地采用，并简化使用其它先进的系统，如外科手术机器人式的方式引导的替代形式发展更好地与现有临床工作流程集成。</p>
<p>出于对对准椎弓根的高精度和融合多个椎骨的手术重复性的要求，这项工作提出了一种通过X射线引导对椎弓根钻探引导器进行机器人定位的方法，从而为外科医生提供了准确放置的端口，其他可以交付手术器械（例如，K线或螺钉）。所提出的方法建立在3D-2D配准算法的基础上，该算法结合了有关设备形状的信息[称为已知组件（KC）]，以从2D射线照片中解决其三维（3D）姿态。除了使用现有的荧光透视工作流程中已获取的图像之外，机器人方法还具有额外的优势，即在成像过程中将外科医生的手保持在X射线视场（FoV）之外。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r10"><sup>10</sup></a> 与幻像和尸体研究一起评估了所提出的方法，并且与更传统的方法（通过光学跟踪器引导机器人定位）相比，对几何精度进行了量化。</p>
<p><a id="sec2" data-feathr-click-track="true"></a></p>
<h2 id="2">2。</h2>
<h2 id="方法">方法</h2>
<p><a id="sec2.1" data-feathr-click-track="true"></a></p>
<h2 id="21">2.1。</h2>
<h3 id="已知组件注册">已知组件注册</h3>
<p>指导的主要手段是最近开发的算法[已知成分配准（KC-Reg）]，该算法使用两步过程对患者解剖结构进行3D–2D配准（以术前CT量表示）。进行2到3幅射线照相（例如，AP，侧面和倾斜），然后注册已知设计的设备，即KC。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r11"><sup>11</sup></a><sup>，</sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r12"><sup>12</sup></a>的解剖结构的登记使用梯度方向（GO）的相似性度量，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r13"><sup>13</sup></a>已经被证明是对信息健壮出现在一个图像中而不是其他（“内容不匹配”，如所述钻具引导器的存在或X射线照片中的椎弓根螺钉，而CT中则没有）。GO由固定（）射线照片中的梯度和移动（）数字重建的射线照片（DRR）图像为</p>
<p><a id="e001" data-feathr-click-track="true"></a></p>
<h2 id="等式-1">等式 （1）</h2>
<p>选择阈值和作为各个图像梯度的中值，而确保只有对齐良好的梯度才有助于度量。DRR是根据患者CT体积（）通过由刚性变换（）确定的线积分（传输/衰减）模拟的
<a id="e002" data-feathr-click-track="true"></a></p>
<h2 id="等式-2">等式 （2）</h2>
<p>公式（1）和（2）定义目标函数如下：
<a id="e003" data-feathr-click-track="true"></a></p>
<h2 id="等式-3">等式 （3）</h2>
<p>协方差矩阵适应性进化策略是一种随机的，无导数的优化方法，用于迭代求解成像设备（C型臂）的坐标系中的患者姿势（），这是由于其强大的收敛特性和并行化的能力。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r14"><sup>14</sup></a></p>
<p><a id="sec2.1.1" data-feathr-click-track="true"></a></p>
<h2 id="211">2.1.1。</h2>
<h4 id="钻探导轨作为已知组件">钻探导轨作为已知组件</h4>
<p>使用<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">图1（a）</a>中<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">所示的</a>简单的椎弓根钻孔导向工具来创建<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">图1和2中</a>的部件计算机辅助设计（CAD）模型（）<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">。</a><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">如图1（b）</a>和<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">1（c）所示</a>，其中工具提示定义为钻具引导器远端孔的质心，并且中心轴与套管对齐。模型中还包括一小部分钻导柄，以说明套管的对称性质并确保3D姿势的适当分辨率。组件的配准使用梯度相关性（GC）相似性度量<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r15"><sup>15，</sup></a>由<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r15"><sup>15</sup></a>和中的正交梯度的归一化互相关（NCC）之和定义。</p>
<p><a id="e004" data-feathr-click-track="true"></a></p>
<h2 id="等式-4">等式 （4）</h2>
<p>以这种方式，GC度量包括高强度KC梯度，同时保留了针对来自解剖结构（例如，椎骨）的图像梯度变化的鲁棒性。设备组件（机器人末端执行器/钻削导向器）的DRR是根据CAD模型生成的体素化模型模拟的。代的公式。（3）和GO的GC，通过优化采集的X射线照片和组件DRR之间的相似性，针对成像设备（）解决了组件的姿势
<a id="e005" data-feathr-click-track="true"></a></p>
<h2 id="等式-5">等式 （5）</h2>
<p><a id="f1" data-feathr-click-track="true"></a></p>
<h2 id="图1-下载">图1 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f001.png">下载</a></h2>
<p>（a）简单的手持式椎弓根钻具导向器，（b）测量的仪器尺寸，（c）KC-Reg组件CAD模型，（d）光学刚体标记器附件，以及（e和f）圆球形的枢轴附件校准。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f001.png" alt="JMI_5_2_021212_f001.png"></p>
<p>结合前一步，可以计算出术前CT扫描坐标中的钻头导向器（组件）姿势的解决方案。</p>
<p><a id="sec2.1.2" data-feathr-click-track="true"></a></p>
<h2 id="212">2.1.2。</h2>
<h4 id="钻探导轨作为跟踪工具">钻探导轨作为跟踪工具</h4>
<p>光学跟踪器（Polaris Vicra，NDI，安大略省滑铁卢）被用作引导机器人的常规替代方法，并被用作与基于KC图像的方法进行比较的基础。为此，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f1">如图1（d）</a>所示，修改了钻头导向器以容纳光学刚体标记。使用球形附件进行工具提示校准以获得平移偏移，然后通过轴校准获得旋转对准。因此，被跟踪工具（）在光学跟踪器（）的坐标系中的最终姿势与KC方法类似。</p>
<p><a id="sec2.2" data-feathr-click-track="true"></a></p>
<h2 id="22">2.2。</h2>
<h3 id="手眼校准">手眼校准</h3>
<p>与外科医生的手眼协调不同，需要附加校准，以使机器人（UR5，Universal Robots，欧登塞，丹麦）的末端执行器与已知（）组件和被跟踪（）组件相关联。这个问题的制定使用了目标工具在多个姿势下的测量结果，该测量来自两个固定的参考坐标系，分别定义为两个框架的手和眼。对于任何两组测量</p>
<p><a id="e006" data-feathr-click-track="true"></a></p>
<h2 id="等式-6">等式 （6）</h2>
<p>因此，这是机器人姿势，特别是从机器人基座到末端执行器（）的变换，并且分别是对应的姿势或，用于KC和跟踪器引导。定义和，手眼校准（）简化为已知问题的解决方案。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r16"><sup>16</sup></a></p>
<p>在下面的实验中，在一个体积内测量了32组随机的末端执行器姿势。由于在离线校准的这一阶段中时间和剂量无关紧要，因此对跟踪器测量值进行平均以减少潜在的抖动，并且KC测量值使用八个径向分布的投影（大于所需的最小两个）来可靠地解决组件姿态。留一法分析用于根据与解决方案的平移偏差评估性能</p>
<p><a id="e007" data-feathr-click-track="true"></a></p>
<h2 id="等式-7">等式 （7）</h2>
<p>其中，未在计算中使用，表示转换矩阵的最后一列（翻译）。每种手眼校准方法均获得了五个独立试验，以评估溶液相对于测量噪声/姿势选择的总体一致性。</p>
<p><a id="sec2.3" data-feathr-click-track="true"></a></p>
<h2 id="23">2.3。</h2>
<h3 id="钻孔导向器的自动对准">钻孔导向器的自动对准</h3>
<p><a id="sec2.3.1" data-feathr-click-track="true"></a></p>
<h2 id="231">2.3.1。</h2>
<h4 id="kc指导">KC指导</h4>
<p>鉴于上述所有先前步骤在该过程之前执行了一次，剩下的术中任务是将机器人驱动到术前CT中定义的计划轨迹。使用三个X射线照片（例如，AP，横向和倾斜），以同时解决和并且在所示变换的链以下<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f2">图2</a>（在也列出<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#t001">表1</a>），所期望的机械手端部执行器的姿势（）可以衍生自初始姿势（）为</p>
<p><a id="e008" data-feathr-click-track="true"></a></p>
<h2 id="等式-8">等式 （8）</h2>
<p><a id="f2" data-feathr-click-track="true"></a></p>
<h2 id="图2-下载">图2 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f002.png">下载</a></h2>
<p>图示了[（b）蓝色]提出的KC的转换树，[[a]红色]图示了用于机器人钻子引导器定位的更常规的跟踪器引导。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f002.png" alt="JMI_5_2_021212_f002.png"></p>
<p><a id="t001" data-feathr-click-track="true"></a></p>
<h2 id="表格1">表格1</h2>
<p>坐标转换中的符号词汇表。</p>
<p>|   | 术前影像量（CT） |
|   | 表示帧相对于 |
|   | X射线成像仪（Siemens Cios alpha C型臂） |
|   | 已知组件（KC指南中的钻探指南提示） |
|   | 跟踪的组件（跟踪器向导中的钻具向导尖端） |
|   | 手眼校正 |
|   | 椎弓根螺钉计划 |
|   | 光学跟踪仪（NDI Polaris Vicra） |
|   | 钻导光学跟踪标记 |
|   | 机器人末端执行器 |
|   | 机器人底座 |</p>
<p>为了达到等式定义的目标姿势。（8），机器人横穿旨在提供安全进入入口点的路径。为此，机器人假定一个中间姿势，该姿势与目标轨迹对齐，但在远离患者的方向上沿轨迹偏移5至10 cm。在此中间姿势下，仅允许机器人沿目标轨迹移动（进近和退回）。</p>
<p><a id="sec2.3.2" data-feathr-click-track="true"></a></p>
<h2 id="232">2.3.2。</h2>
<h4 id="追踪器指导">追踪器指导</h4>
<p>可以为跟踪器指导方案类似地定义类似的表达式</p>
<p><a id="e009" data-feathr-click-track="true"></a></p>
<h2 id="等式-9">等式 （9）</h2>
<p>其中，是相对于光学跟踪器（）的刚性连接参考标记的姿态，并且是跟踪器对术前CT图像的基准配准。请注意，这两个变换在KC指南中仅被替换，该指南随每次新的射线照相图像采集和配准而更新。</p>
<p><a id="sec2.4" data-feathr-click-track="true"></a></p>
<h2 id="24">2.4。</h2>
<h3 id="系统整合">系统整合</h3>
<p><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f3">图3</a>显示了TREK外科手术导航平台，该平台用于协调各个组件，以提供KC和跟踪器引导的机器人钻探引导器定位。下面总结了<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r17"><sup>17个</sup></a>单独的模块，以处理与每个组件相关的任务（例如，成像设备，机械臂，3D–2D注册和跟踪）。</p>
<p><a id="f3" data-feathr-click-track="true"></a></p>
<h2 id="图3-下载">图3 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f003.png">下载</a></h2>
<p>在尸体实验期间基于TREK定制的手术导航解决方案的屏幕截图。专用模块显示在左侧。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f003.png" alt="JMI_5_2_021212_f003.png"></p>
<p>所述_椎弓根计划_模块允许轨迹术前CT内，如图绿色在规划<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f3">图3中</a>。该_3D-2D登记_模块使用患者和组件注册的快速计算GPU加速的程序。所述_C型臂_与成像系统模块处理通信（CIOS阿尔法，西门子Healthineers，德国埃尔兰根），因为它们被获取，其自动读取的X射线图像和查询实时C形臂编码器的位置。同样，_跟踪_模块查询可实时从跟踪器转换测量结果。跟踪器的基准配准可以通过允许在每个基准草稿处（手动预先划分）进行跟踪的指针工具的测量的功能轻松进行。最后，与机器人臂的通信是通过实现_UR5_模块，在所示作为活性<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f3">图3</a>。模块之间的事件驱动通信产生了任务的简化和自动执行，这有助于执行以下描述的实验。</p>
<p><a id="sec2.5" data-feathr-click-track="true"></a></p>
<h2 id="25">2.5。</h2>
<h3 id="幻影和尸体中的实验">幻影和尸体中的实验</h3>
<p>首先在幻影实验中评估了该方法的可行性和准确性，针对的是跨越T1-L5的17个椎弓根轨迹（<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f4">图4</a>）。椎弓根平面图是在CT图像中手动定义的，因此每个轨迹都具有解剖上切合实际的入口点，并横穿椎弓根通道的中心。轨迹规划的自动化是进一步简化流程的其他工作的主题。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r18"><sup>18 </sup></a><sup>– </sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r23"><sup>23</sup></a></p>
<p><a id="f4" data-feathr-click-track="true"></a></p>
<h2 id="图4-下载">图4 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f004.png">下载</a></h2>
<p>幻影和尸体研究。（a）Sawbones脊柱体模实验设置，（b）感兴趣体模区域的放大，（c）感兴趣尸体区域的放大，以及（d）尸体实验设置。跟踪器未显示。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f004.png" alt="JMI_5_2_021212_f004.png"></p>
<p>对于每个轨迹，将机器人握持的钻具引导器放置在靠近目标平面的位置，以便在X射线FoV中捕获骨骼解剖结构和组件。在获取射线照片并通过KC-Reg进行配准之后，通过KC引导将机器人驱动到计划的轨迹，并在目标处获取了锥束CT（CBCT）扫描以进行离线误差分析。返回相同的初始姿势，重复相同的步骤以进行跟踪器引导。对于尸体实验，暴露了腰椎，并瞄准了跨越L3–L5的6条轨迹。请注意，尽管为方便起见获取了AP，侧面和倾斜X线照片，但不需要精确的视图，并且在先前的研究中，每个视图之间的15度分隔被证明是准确的。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r24"><sup>24</sup></a> 轨迹计划和真相定义如上所述，用于幻像研究。</p>
<p>该系统的准确度在目标配准误差（TRE）在所述钻具引导尖端（来定量），并在主成分轴对准目标计划（）</p>
<p><a id="e010" data-feathr-click-track="true"></a></p>
<h2 id="等式-10">等式 （10）</h2>
<p>其中转换矩阵的最后一列（）是工具提示的平移，而第三列（或旋转矩阵的最后一列）定义了组件的主轴（）。患者和部件的真实姿势分别通过将CT体积（）和部件模型（）进行CBCT图像重建的独立3D–3D配准确定。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">图中</a>的错误窗口<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">。</a><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">图7（c）</a>和<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f8">8（c）</a>示出了围绕由平均值和钻头引导对准确定的计划轨迹的螺钉放置范围。窗口在各个方向上偏移了一定距离，并在螺钉入口点周围旋转了。</p>
<p><a id="sec3" data-feathr-click-track="true"></a></p>
<h2 id="3">3。</h2>
<h2 id="结果">结果</h2>
<p><a id="sec3.1" data-feathr-click-track="true"></a></p>
<h2 id="31">3.1。</h2>
<h3 id="评估手眼校准">评估手眼校准</h3>
<p>尽管使用光学跟踪器已经为机器人控制建立了良好的手眼校准功能，但是在基于图像的指导下进行手眼校准仍需要进一步的研究。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f5">图5（a）</a>总结了对问题<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r25"><sup>25 </sup></a><sup>– </sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r32"><sup>32</sup></a>的不同求解器的评估，具体方法如第15节所述，计算的留一法误差（）的平移分量的分布。 <a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#sec2.2">2.2</a>。结果表明，可分离求解器在这方面提供了卓越的性能。使用Park等人的求解器，观察到为[ <a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f5">图5（b）</a> ]。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f5">图5（c）中</a>重复试验的交叉验证结果显示，与跟踪器（）方法相比，KC方法（）的手眼校准之间平移差异（）的变化性降低，突出了前者的解决方案一致性有所提高。改善KC方法一致性的一个可能解释可能是其捕获更多种钻具导向姿势的能力，请参见。被动标记，仅从面向跟踪器的一侧可见。</p>
<p><a id="f5" data-feathr-click-track="true"></a></p>
<h2 id="图5-下载">图5 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f005.png">下载</a></h2>
<p>（a）通过（从左到右）Tsai，Horaud，Park，Chou，Andreff，Daniilidis，Shiu和Wang等人，求解器的平移一劳永逸误差分布，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r25"><sup>25 </sup></a><sup>– </sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r32"><sup>32</sup></a>（b）误差分布（）对于KC方法的手眼校准显示为小提琴图，其中阴影区域是对基本采样点的概率密度函数的估计，以及（c）跟踪仪的手眼校准试验之间的平移差异（上方）左）和KC（右下）方法。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f005.png" alt="JMI_5_2_021212_f005.png"></p>
<p><a id="sec3.2" data-feathr-click-track="true"></a></p>
<h2 id="32">3.2。</h2>
<h3 id="幻影研究解决方案的局部性">幻影研究：解决方案的局部性</h3>
<p>通过将光学参考标记放置在L5附近的幻影上（在临床实践中也很常见），对于跟踪器引导更远离参考标记（即，从L5开始）的轨迹，预期TRE的趋势将增加。图<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f6">6（a）</a>显示，在距腰椎的距离增加的情况下，跟踪器的TRE的平移部分会发生这种退化。由于新注册患者的注册更新，KC指南显示出对此类效果的鲁棒性。</p>
<p><a id="f6" data-feathr-click-track="true"></a></p>
<h2 id="图6-下载">图6 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f006.png">下载</a></h2>
<p>（a）  和（b）  相对于脊柱体模中的目标椎骨进行两种形式的引导。观察到KC进近可以在多个椎骨水平上一致地实现目标位置和进近角。阴影区域表示回归线的95％置信区间。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f006.png" alt="JMI_5_2_021212_f006.png"></p>
<p>在脊柱体模实验中，KC引导产生了和，而跟踪器引导产生了和，如图5 和6 所示<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">。</a><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">7（a）</a>和<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">7（b）</a>。尽管结果总体上可比，但配对测试表明，在KC指导下，统计学上的显着降低（）误差。可能的原因是腰椎区域周围（仅）注册基准的（有意的）空间分布，因为这种有限的覆盖范围在正常实践中相当普遍。仍然，两种方法都提供可接受的误差（椎弓根皮层内的理论位置），如图<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f7">7（c）所示</a>。</p>
<p><a id="f7" data-feathr-click-track="true"></a></p>
<h2 id="图7-下载">图7 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f007.png">下载</a></h2>
<p>幻象研究中机器人制导的几何精度：（a）  和（b）  对于KC和跟踪器制导方法，（c）L4中相对于螺钉的计划轨迹的误差窗口（如第<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#sec2.5">2.5</a>节所述计算  ）（绿色） ）基于KC（青色）和跟踪器（洋红色）指南的平均误差。计划的螺钉入口点标有一个点。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f007.png" alt="JMI_5_2_021212_f007.png"></p>
<p><a id="sec3.3" data-feathr-click-track="true"></a></p>
<h2 id="33">3.3。</h2>
<h3 id="尸体研究变形的鲁棒性">尸体研究：变形的鲁棒性</h3>
<p>尸体在CT扫描仪和C型臂/手术台之间的处理和运输过程中均发生了实际的大体解剖变形，每种情况下都倾向于放置。导致尸体中机器人的KC引导，如图1 和2 所示<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f8">。</a><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f8">8（a）</a>和<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f8">8（b）</a>略高于模型中的TRE。尽管由于软组织变形而产生的梯度以及由于较高的衰减（特别是在侧视图中）而导致的图像质量降低提出了挑战，但该方法成功地将骨皮质内的轨迹保持为如图<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#f8">8（c）所示。</a> 。尽管在转化及对表中的试样的放置护理，从术前CT扫描时的引起的变形和实验导致显著（与呈现的跟踪器引导时）更高的错误的和。值得注意的是，在术前CT和实验时间之间，注册基准也受到了变形，从而加剧了跟踪器注册的错误。相比之下，KC方法对于这种变化仍然保持稳健，因为它依赖于相对刚性的结构，至少在放射线FoV的局部区域内（给定放射线视图中捕获了三到四个椎骨）。</p>
<p><a id="f8" data-feathr-click-track="true"></a></p>
<h2 id="图8-下载">图8 <a href="https://www.spiedigitallibrary.org/proceedings/DownloadFigures?url=/ContentImages/Journals/JMIOBU/5/2/021212/FigureImages/JMI_5_2_021212_f008.png">下载</a></h2>
<p>尸体研究中机器人定位的几何精度：（a）  和（b）  对于KC和跟踪器引导的方法，（c ）基于KC（青色）的平均误差，L4中相对于螺钉计划轨迹的误差窗口（绿色） ）和跟踪器（洋红色）指南。计划的螺钉入口点标有一个点。</p>
<p><img src="https://www.spiedigitallibrary.org/ContentImages/Journals/JMIOBU/5/2/021212/WebImages/JMI_5_2_021212_f008.png" alt="JMI_5_2_021212_f008.png"></p>
<p><a id="sec4" data-feathr-click-track="true"></a></p>
<h2 id="4">4。</h2>
<h2 id="结论">结论</h2>
<p>提出了一种通过KC-Reg机器人定位椎弓根钻头的方法，该方法结合了典型手术流程中已经获得的荧光透视图，并结合了患者解剖结构（术前CT）和器械形状（已知组件模型）的先验知识。透视引导下的整形外科或神经脊柱外科手术的常规做法涉及多平面X线照相，由外科医生定性解释以评估手术器械的3D放置（例如，确定椎弓根是否破裂），例如，PA和LAT视野以评估位置和深度分别在椎弓根中。KC方法是从相同的多平面视图或角度间隔最小的视图中实现的（<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r24"><sup>24）</sup></a>—提供准确，定量，可重现且较少受个别外科医生的专业知识和定性图像解释约束的指导和质量保证。</p>
<p>在模拟椎弓根螺钉放置的体模和尸体实验中，机器人的KC引导表现出可比性，并且在整体解剖变形的条件下，其性能优于跟踪器引导。这意味着，由于在荧光透视FoV中使用了局部图像特征，因此与传统的基于跟踪器的引导相比，KC引导更不易受到解剖变形的影响。但是，由于无法解释患者在射线照相视图之间的运动而导致的不足。即使像此处进行的实验一样，每个轨迹都有单独的配准，配准后的患者移动在驱动机器人时也无法说明。此外，成像和配准所需的时间建议在指导而非实时指导中进行点检/逐步拍摄反馈。例如，像在当前的透视引导手术工作流程中一样，外科医生可以在每个跨蒂路径上的特定步骤（例如，刚进入后，在中点和/或在轨迹的终点）获取射线照相视图。相比之下，光学跟踪器提供实时反馈并通过光学参考标记说明刚体的运动，但是它们很容易在手术过程中产生解剖学变形。因此，这项工作中提出的发现表明，将KC引导方法与光学跟踪器结合起来以利用两者的优势的可能性。光学跟踪器提供实时反馈，并通过光学参考标记说明刚体的运动，但是它们很容易在手术过程中产生解剖学变形。因此，这项工作中提出的发现表明，将KC引导方法与光学跟踪器结合起来以利用两者的优势的可能性。光学跟踪器提供实时反馈，并通过光学参考标记说明刚体的运动，但是它们很容易在手术过程中产生解剖学变形。因此，这项工作中提出的发现表明，将KC引导方法与光学跟踪器结合起来以利用两者的优势的可能性。</p>
<p>一种可能的方法是使用KC-Reg更新跟踪器注册。这将要求被跟踪的工具和KC处于同一参照系中，这样。由于被跟踪的工具校准仅考虑工具提示和套管的方向，因此将为跟踪器任意定义围绕套管的旋转。解决缺少的自由度的一种直接方法是使用精心获得的手眼校准，例如。然后，可以将所报告的KC和跟踪器方法的工具姿势之间的差异归因于患者变形和/或参考标记运动，从而描述了（在跟踪器注册期间使用的，在手术之前使用的）当前电流之间的局部刚性偏移。，预期包括变形。该偏移量可用于检测解剖结构变形并警告外科医生跟踪器配准已过时，甚至可将其更新为，然后可以将其代入等式。（9）像以前一样驱动机器人。</p>
<p>机器人定位的KC指导似乎很有希望，与荧光检查的工作流程一致，并且可以帮助减少对患者（在试验和错误/定性解释中获得的荧光检查更少）和外科手术人员（使用机器人末端执行器）的术中放射剂量。而不是外科医生的手-在射线照相FoV附近）。应该承认在未来工作中需要改进的各个领域。物理模型和虚拟模型之间的差异会在KC手眼校准过程和钻头导向姿态的KC-Reg计算过程中增加误差。通过更好地建模组件（例如，使用激光扫描或供应商提供的规范）或通过使用3D打印的定制制造，可以达成更高级别的协议。在这项工作中评估的手眼校准解算器包括可分离和同时分类的子集。但是，迭代求解器可以提供较低的误差。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r16"><sup>可以采用16种</sup></a>先进的路径规划方法，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r33"><sup> 33种</sup></a>碰撞检测，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r34"><sup> 34种</sup></a>和虚拟固定装置<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r35"><sup> 35</sup></a>来提高机器人运动期间的安全性。最后，为了进一步简化工作流程，可以改善3D-2D注册步骤的运行时间。当前的实现方式是，初始患者注册（CT到荧光透视）需要120 s的时间来计算，从而解决了广阔的搜索空间和多次启动的问题，从而确保了解剖结构的准确注册。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r36"><sup>36</sup></a>后续的KC注册目前未利用此初始化功能，因此需要进行类似的操作到180秒，包括附加组件注册步骤。工作流的含义是，一次（或不频繁）执行初始注册，并且仅需要KC注册的次数就可以更新机器人的潜在移动（例如，每隔一个椎骨水平一次）。因此，可以通过更好地利用这种广泛的初始化，使用具有更快收敛时间的替代（本地）优化器，<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r37"><sup>37</sup></a>和使用性能更高（或多个）的GPU 来改善KC注册运行时。</p>
<p>在这项工作中使用的已知组件配准的另一种方法是为执行器配备牢固地固定在手术器械上的专用不透射线基准标记（也是已知配置）。<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r38"><sup>38</sup></a><sup>，</sup><a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r39"><sup>39</sup></a>这种方法可能是有利的，因为可以针对3D–2D配准的任务优化基准标记，从而潜在地增加其醒目性，在单个视图中定位的能力以及配准的速度。但是，KC方法的优点是无需额外的标记即可进行操作并直接使用机器人末端执行器。首先，它避免了对额外工具的需求以及所需的处置或灭菌成本。其次，它保持了效应器的原始预期设计（例如，钻具导向器的设计），因此结构紧凑，减少了碰撞的风险。最终，KC方法将校准步骤的数量减少到机器人对其末端执行器的手眼校准的步骤，而不透射线的基准方法将需要额外的（基准到执行器）工具提示校准。</p>
<p>尽管本研究的重点是脊柱手术中的机器人辅助，但也可以设想在其他程序中对机器人定位进行KC指导。例如，立体脑电图放置的KC指导不仅可以使机器人定位<a href="https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-5/issue-2/021212/Robotic-drill-guide-positioning-using-known-component-3D2D-image-registration/10.1117/1.JMI.5.2.021212.full#r40"><sup>40</sup></a>受益，而且脑电图引线自身的放置也可以受益（作为KC）。同样，该方法可以扩大机器人辅助在当前严重依赖透视检查的手术中的应用范围，并且需要外科医生方面的高度专业知识才能将K线置于复杂的解剖形状中，例如骨盆创伤手术。</p>
<p><a id="ID0E5FBG" data-feathr-click-track="true"></a></p>
<h2 id="披露事项">披露事项</h2>
<p>这项研究得到了美国国立卫生研究院（NIH）资助编号R01-EB-017226，美国国立卫生研究院（NIH）培训资助5号T32 AR067708-03的支持，以及与Siemens Healthineers（德国埃尔兰根）的学术和产业合作。作者没有宣布任何经济或其他方面的利益冲突。</p>
<p><a id="ID0ECGBG" data-feathr-click-track="true"></a></p>
<h2 id="致谢">致谢</h2>
<p>作者感谢Drs。Sebastian Vogt，Gerhard Kleinszig，Markus Weiten和Wei Wei（西门子医疗公司）为这项工作中使用的移动C型臂提供了宝贵的帮助。作者还要感谢Ronn Wade先生（马里兰大学解剖学委员会）和Rajiv Iyer博士（约翰霍普金斯大学神经外科）对尸体标本的帮助，Matthew Jacobson博士（约翰霍普金斯大学生物医学工程）对C-的帮助。手臂校准和CT重建，Alex Alex先生（约翰霍普金斯大学生物医学工程）负责钻探引导机器人附件的设计和制造。</p>
<p><a id="ID0EFGBG" data-feathr-click-track="true"></a>
<a id="ID0EFGBG" data-feathr-click-track="true"></a></p>
<h2 id="参考资料">参考资料</h2>
<p>1。 <a id="r1" data-feathr-click-track="true"></a></p>
<p>AJ Weiss等人，《美国医院手术室程序的特征》，2011年：HCUP统计简报＃170，医疗研究与质量机构，马里兰州罗克维尔（2014）。<a href="http://scholar.google.com/scholar_lookup?&amp;author=A.+J.+Weiss&amp;publication_year=2014">谷歌学术</a></p>
<p>2。 <a id="r2" data-feathr-click-track="true"></a></p>
<p>F. Perna等人，“ <strong><em>椎弓根螺钉插入技术：文献的更新和回顾</em></strong> ”，肌肉骨骼外科，100（3），165 –169（2016）。<a href="http://dx.doi.org/10.1007/s12306-016-0438-8">http://dx.doi.org/10.1007/s12306-016-0438-8 </a><a href="http://scholar.google.com/scholar_lookup?title=Pedicle+screw+insertion+techniques:+an+update+and+review+of+the+literature&amp;author=F.+Perna&amp;journal=Musculoskeletal+Surg.&amp;volume=100&amp;issue=3&amp;publication_year=2016&amp;pages=165-169">Google学术搜索</a></p>
<p>3。 <a id="r3" data-feathr-click-track="true"></a></p>
<p>A. Attar等人，“ <strong><em>腰椎椎弓根：外科手术解剖学评估和关系</em></strong> ”，Eur。脊柱J.，10（1），10-15（2001）。<a href="http://dx.doi.org/10.1007/s005860000198">http://dx.doi.org/10.1007/s005860000198 </a><a href="http://scholar.google.com/scholar_lookup?title=Lumbar+pedicle:+surgical+anatomic+evaluation+and+relationships&amp;author=A.+Attar&amp;journal=Eur.+Spine+J.&amp;volume=10&amp;issue=1&amp;publication_year=2001&amp;pages=10-15">Google学术搜索</a></p>
<p>4。 <a id="r4" data-feathr-click-track="true"></a></p>
<p>OP Gautschi等人，“ <strong><em>胸腰椎外科手术中与椎弓根螺钉置入有关的临床相关并发症及其处理：对35，630根椎弓根螺钉的文献综述</em></strong>，” Neurosurg。聚焦，31（4），E8（2011）。<a href="http://dx.doi.org/10.3171/2011.7.FOCUS11168">http://dx.doi.org/10.3171/2011.7.FOCUS11168 </a><a href="http://scholar.google.com/scholar_lookup?title=Clinically+relevant+complications+related+to+pedicle+screw+placement+in+thoracolumbar+surgery+and+their+management:+a+literature+review+of+35,+630+pedicle+screws&amp;author=O.+P.+Gautschi&amp;journal=Neurosurg.+Focus&amp;volume=31&amp;issue=4&amp;publication_year=2011&amp;pages=E8">Google学术搜索</a></p>
<p>5， <a id="r5" data-feathr-click-track="true"></a></p>
<p>A. Amr，A。Giese和SR Kantelhardt，“ <strong><em>脊柱导航和机器人辅助手术：历史回顾和最新技术</em></strong> ”，Rob。外科。2014（1），19 –26（2014）。<a href="http://dx.doi.org/10.2147/RSRR.S54390">http://dx.doi.org/10.2147/RSRR.S54390 </a><a href="http://scholar.google.com/scholar_lookup?title=Navigation+and+robot-aided+surgery+in+the+spine:+historical+review+and+state+of+the+art&amp;author=A.+Amr&amp;author=A.+Giese&amp;author=S.+R.+Kantelhardt&amp;journal=Rob.+Surg.&amp;volume=2014&amp;issue=1&amp;publication_year=2014&amp;pages=19-26">Google学术搜索</a></p>
<p>6。 <a id="r6" data-feathr-click-track="true"></a></p>
<p>F. Roser，M。Tatagiba和G. Maier，“ <strong><em>脊柱机器人技术：当前的应用和未来的前景</em></strong> ”，Neurosurgery，72（增刊1），12-18（2013年）。<a href="http://dx.doi.org/10.1227/NEU.0b013e318270d02c">http://dx.doi.org/10.1227/NEU.0b013e318270d02c</a> NEQUEB <a href="http://scholar.google.com/scholar_lookup?title=Spinal+robotics:+current+applications+and+future+perspectives&amp;author=F.+Roser&amp;author=M.+Tatagiba&amp;author=G.+Maier&amp;journal=Neurosurgery&amp;volume=72&amp;issue=Suppl.%201&amp;publication_year=2013&amp;pages=12-18">Google学术搜索</a></p>
<p>7。 <a id="r7" data-feathr-click-track="true"></a></p>
<p>V. Kosmopoulos和C. Schizas，“ <strong><em>椎弓根螺钉放置的准确性：一项荟萃分析</em></strong> ”，Spine，32（3），E111 –E120（2007）。<a href="http://dx.doi.org/10.1097/01.brs.0000254048.79024.8b">http://dx.doi.org/10.1097/01.brs.0000254048.79024.8b</a> SPINDD 0362-2436 <a href="http://scholar.google.com/scholar_lookup?title=Pedicle+screw+placement+accuracy:+a+meta-analysis&amp;author=V.+Kosmopoulos&amp;author=C.+Schizas&amp;journal=Spine&amp;volume=32&amp;issue=3&amp;publication_year=2007&amp;pages=E111-E120">Google学术搜索</a></p>
<p>8。 <a id="r8" data-feathr-click-track="true"></a></p>
<p>T. Koivukangas，JP Katisko和JP Koivukangas，“ <strong><em>光学和电磁跟踪系统的技术准确性</em></strong> ”，SpringerPlus，2（1），90（2013）。<a href="http://dx.doi.org/10.1186/2193-1801-2-90">http://dx.doi.org/10.1186/2193-1801-2-90 </a><a href="http://scholar.google.com/scholar_lookup?title=Technical+accuracy+of+optical+and+the+electromagnetic+tracking+systems&amp;author=T.+Koivukangas&amp;author=J.+P.+Katisko&amp;author=J.+P.+Koivukangas&amp;journal=SpringerPlus&amp;volume=2&amp;issue=1&amp;publication_year=2013&amp;pages=90">Google学术搜索</a></p>
<p>9。 <a id="r9" data-feathr-click-track="true"></a></p>
<p>RL加洛韦，密歇根州米加，“ <strong><em>器官变形和导航”</em></strong>，《现代手术室中的成像和可视化：医师综合指南》，纽约州史普林格121-132（2015年）。<a href="http://dx.doi.org/10.1007/978-1-4939-2326-7_9">http://dx.doi.org/10.1007/978-1-4939-2326-7_9 </a><a href="http://scholar.google.com/scholar_lookup?title=Organ+deformation+and+navigation&amp;author=R.+L.+Galloway&amp;author=M.+I.+Miga&amp;publication_year=2015&amp;pages=121-132">Google学术搜索</a></p>
<p>10。 <a id="r10" data-feathr-click-track="true"></a></p>
<p>RH Taylor和D. Stoianovici，“ <strong><em>计算机集成手术中的医疗机器人技术</em></strong> ”，IEEE Trans。抢。自动，19（5），765 –781（2003）。<a href="http://dx.doi.org/10.1109/TRA.2003.817058">http://dx.doi.org/10.1109/TRA.2003.817058</a> IRAUEZ 1042-296X <a href="http://scholar.google.com/scholar_lookup?title=Medical+robotics+in+computer-integrated+surgery&amp;author=R.+H.+Taylor&amp;author=D.+Stoianovici&amp;journal=IEEE+Trans.+Rob.+Autom.&amp;volume=19&amp;issue=5&amp;publication_year=2003&amp;pages=765-781">Google学术搜索</a></p>
<p>11。 <a id="r11" data-feathr-click-track="true"></a></p>
<p>A. Uneri等人，“ <strong><em>已知组件3D–2D注册以确保脊柱外科手术椎弓根螺钉放置的质量</em></strong> ”，物理。中 生物化学杂志，60（20），8007 –8024（2015）。<a href="http://dx.doi.org/10.1088/0031-9155/60/20/8007">http://dx.doi.org/10.1088/0031-9155/60/20/8007</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=Known-component+3D%E2%80%932D+registration+for+quality+assurance+of+spine+surgery+pedicle+screw+placement&amp;author=A.+Uneri&amp;journal=Phys.+Med.+Biol.&amp;volume=60&amp;issue=20&amp;publication_year=2015&amp;pages=8007-8024">Google学术搜索</a></p>
<p>12 <a id="r12" data-feathr-click-track="true"></a></p>
<p>A. Uneri等人，“ <strong><em>使用已知组件的3D–2D图像配准在脊柱外科手术中对器械放置进行术中评估</em></strong> ”，Phys。中 生物化学，62（8），3330 –3351（2017）。<a href="http://dx.doi.org/10.1088/1361-6560/aa62c5">http://dx.doi.org/10.1088/1361-6560/aa62c5</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=Intraoperative+evaluation+of+device+placement+in+spine+surgery+using+known-component+3D%E2%80%932D+image+registration&amp;author=A.+Uneri&amp;journal=Phys.+Med.+Biol.&amp;volume=62&amp;issue=8&amp;publication_year=2017&amp;pages=3330-3351">Google学术搜索</a></p>
<p>13 <a id="r13" data-feathr-click-track="true"></a></p>
<p>T. De Silva等人，“ <strong><em>3D–2D图像配准用于脊柱手术中的目标定位：相似度指标的研究可为内容不匹配提供鲁棒性</em></strong>，” Phys。中 生物化学，61（8），3009 – 3025（2016）。<a href="http://dx.doi.org/10.1088/0031-9155/61/8/3009">http://dx.doi.org/10.1088/0031-9155/61/8/3009</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=3D%E2%80%932D+image+registration+for+target+localization+in+spine+surgery:+investigation+of+similarity+metrics+providing+robustness+to+content+mismatch&amp;author=T.+De+Silva&amp;journal=Phys.+Med.+Biol.&amp;volume=61&amp;issue=8&amp;publication_year=2016&amp;pages=3009-3025">Google学术搜索</a></p>
<p>14。 <a id="r14" data-feathr-click-track="true"></a></p>
<p>N. Hansen和A. Ostermeier，“ <strong><em>进化策略中完全去随机化的自我适应</em></strong> ”，Evol。计算（9）（2），159 –195（2001）。<a href="http://dx.doi.org/10.1162/106365601750190398">http://dx.doi.org/10.1162/106365601750190398</a> EOCMEO <a href="http://scholar.google.com/scholar_lookup?title=Completely+derandomized+self-adaptation+in+evolution+strategies&amp;author=N.+Hansen&amp;author=A.+Ostermeier&amp;journal=Evol.+Comput.&amp;volume=9&amp;issue=2&amp;publication_year=2001&amp;pages=159-195">谷歌学术</a></p>
<p>15 <a id="r15" data-feathr-click-track="true"></a></p>
<p>GP Penney等人，“ <strong><em>用于2-D–3-D医学图像配准的相似性度量的比较</em></strong> ”，IEEE Trans。中 影像学，17（4），586 –595（1998）。<a href="http://dx.doi.org/10.1109/42.730403">http://dx.doi.org/10.1109/42.730403</a> ITMID4 0278-0062 <a href="http://scholar.google.com/scholar_lookup?title=A+comparison+of+similarity+measures+for+use+in+2-D%E2%80%933-D+medical+image+registration&amp;author=G.+P.+Penney&amp;journal=IEEE+Trans.+Med.+Imaging&amp;volume=17&amp;issue=4&amp;publication_year=1998&amp;pages=586-595">Google学术搜索</a></p>
<p>16。 <a id="r16" data-feathr-click-track="true"></a></p>
<p>M. Shah等人，Proc.Natl.Acad.Sci.USA，87：“ <strong><em>用于感知系统评估的机器人传感器校准方法概述</em></strong> ”。智能系统研讨会绩效指标（PERMIS 2012），15（2012）。<a href="http://dx.doi.org/10.1145/2393091.2393095">http://dx.doi.org/10.1145/2393091.2393095 </a><a href="http://scholar.google.com/scholar_lookup?title=An+overview+of+robot-sensor+calibration+methods+for+evaluation+of+perception+systems&amp;author=M.+Shah&amp;conference=Proc.+Workshop+Performance+Metrics+for+Intelligent+Systems+(PERMIS+2012)&amp;publication_year=2012&amp;pages=15">Google学术搜索</a></p>
<p>17。 <a id="r17" data-feathr-click-track="true"></a></p>
<p>A. Uneri等人，“ <strong><em>TREK：用于术中锥形束CT引导手术的集成系统架构</em></strong> ”，J.计算机 助攻。收音机 外科杂志，7（1），159 –173（2012）。<a href="http://dx.doi.org/10.1007/s11548-011-0636-7">http://dx.doi.org/10.1007/s11548-011-0636-7 </a><a href="http://scholar.google.com/scholar_lookup?title=TREK:+an+integrated+system+architecture+for+intraoperative+cone-beam+CT-guided+surgery&amp;author=A.+Uneri&amp;journal=Int.+J.+Comput.+Assist.+Radiol.+Surg.&amp;volume=7&amp;issue=1&amp;publication_year=2012&amp;pages=159-173">Google学术搜索</a></p>
<p>18岁 <a id="r18" data-feathr-click-track="true"></a></p>
<p>J. Goerres等人，“ <strong><em>使用可变形图谱套准进行椎弓根螺钉计划</em></strong> ”，物理 中 生物化学，62（7），2871 –2891（2017）。<a href="http://dx.doi.org/10.1088/1361-6560/aa5f42">http://dx.doi.org/10.1088/1361-6560/aa5f42</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=Spinal+pedicle+screw+planning+using+deformable+atlas+registration&amp;author=J.+Goerres&amp;journal=Phys.+Med.+Biol.&amp;volume=62&amp;issue=7&amp;publication_year=2017&amp;pages=2871-2891">Google学术搜索</a></p>
<p>19 <a id="r19" data-feathr-click-track="true"></a></p>
<p>R. Wicker和B. Tedla，“ <strong><em>从患者数据中自动确定椎弓根螺钉的大小，长度和轨迹</em></strong> ”，在Conf。程序 IEEE医学与生物工程学会，1487年– 1490年（2004年）。<a href="http://dx.doi.org/10.1109/IEMBS.2004.1403457">http://dx.doi.org/10.1109/IEMBS.2004.1403457 </a><a href="http://scholar.google.com/scholar_lookup?title=Automatic+determination+of+pedicle+screw+size,+length,+and+trajectory+from+patient+data&amp;author=R.+Wicker&amp;author=B.+Tedla&amp;conference=Conf.+Proc.+IEEE+Engineering+in+Medicine+and+Biology+Society&amp;publication_year=2004&amp;pages=1487-1490">Google学术搜索</a></p>
<p>20 <a id="r20" data-feathr-click-track="true"></a></p>
<p>J. Lee等人，“ <strong><em>考虑到操作安全性和椎骨螺钉界面强度的最佳腰椎融合手术规划指导</em></strong>，”，《国际**<em>口腔医学杂志</em>**》。J. Med。机器人。计算 助攻。外科。8（3），261 –272（2012）。<a href="http://dx.doi.org/10.1002/rcs.1413">http://dx.doi.org/10.1002/rcs.1413 </a><a href="http://scholar.google.com/scholar_lookup?title=Optimal+surgical+planning+guidance+for+lumbar+spinal+fusion+considering+operational+safety+and+vertebra-screw+interface+strength&amp;author=J.+Lee&amp;journal=Int.+J.+Med.+Robot.+Comput.+Assist.+Surg.&amp;volume=8&amp;issue=3&amp;publication_year=2012&amp;pages=261-272">Google学术搜索</a></p>
<p>21 <a id="r21" data-feathr-click-track="true"></a></p>
<p>N. Daemi et al。，Proc.Natl.Acad.Sci.USA，90：5873-5877，“ <strong><em>使用术前CT图像规划腰椎融合术中的螺钉插入轨迹</em></strong> ”。年度诠释 Conf。IEEE医学与生物学工程学会（EMBS 2015），3639 –3642（2015）。<a href="http://dx.doi.org/10.1109/EMBC.2015.7319181">http://dx.doi.org/10.1109/EMBC.2015.7319181 </a><a href="http://scholar.google.com/scholar_lookup?title=Planning+screw+insertion+trajectory+in+lumbar+spinal+fusion+using+pre-operative+CT+images&amp;author=N.+Daemi&amp;conference=Proc.+of+the+Annual+Int.+Conf.+of+the+IEEE+Engineering+in+Medicine+and+Biology+Society+(EMBS+2015)&amp;publication_year=2015&amp;pages=3639-3642">Google学术搜索</a></p>
<p>22 <a id="r22" data-feathr-click-track="true"></a></p>
<p>D. Knez等人，“ <strong><em>椎弓根螺钉置入手术的计算机辅助螺钉尺寸和插入轨迹计划</em></strong> ”，IEEE Trans。中 成像，35（6），1420–1430（2016）。<a href="http://dx.doi.org/10.1109/TMI.2016.2514530">http://dx.doi.org/10.1109/TMI.2016.2514530</a> ITMID4 0278-0062 <a href="http://scholar.google.com/scholar_lookup?title=Computer-assisted+screw+size+and+insertion+trajectory+planning+for+pedicle+screw+placement+surgery&amp;author=D.+Knez&amp;journal=IEEE+Trans.+Med.+Imaging&amp;volume=35&amp;issue=6&amp;publication_year=2016&amp;pages=1420-1430">Google学术搜索</a></p>
<p>23。 <a id="r23" data-feathr-click-track="true"></a></p>
<p>GF Solitro和F. Amirouche，“ <strong><em>开发用于椎弓根螺钉放置的计算机辅助算法的创新方法</em></strong> ”，医学。。物理学报，38（4），354 –365（2016）。<a href="http://dx.doi.org/10.1016/j.medengphy.2016.01.005">http://dx.doi.org/10.1016/j.medengphy.2016.01.005</a> MEPHEO 1350-4533 <a href="http://scholar.google.com/scholar_lookup?title=Innovative+approach+in+the+development+of+computer+assisted+algorithm+for+spine+pedicle+screw+placement&amp;author=G.+F.+Solitro&amp;author=F.+Amirouche&amp;journal=Med.+Eng.+Phys.&amp;volume=38&amp;issue=4&amp;publication_year=2016&amp;pages=354-365">Google学术搜索</a></p>
<p>24 <a id="r24" data-feathr-click-track="true"></a></p>
<p>A. Uneri等人，“ <strong><em>3D–2D手术指导配准：投影视角对配准精度的影响</em></strong> ”，物理。中 生物化学，59（2），271 –287（2014）。<a href="http://dx.doi.org/10.1088/0031-9155/59/2/271">http://dx.doi.org/10.1088/0031-9155/59/2/271</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=3D%E2%80%932D+registration+for+surgical+guidance:+effect+of+projection+view+angles+on+registration+accuracy&amp;author=A.+Uneri&amp;journal=Phys.+Med.+Biol.&amp;volume=59&amp;issue=2&amp;publication_year=2014&amp;pages=271-287">Google学术搜索</a></p>
<p>25岁 <a id="r25" data-feathr-click-track="true"></a></p>
<p>RY Tsai和RK Lenz，“ <strong><em>一种用于全自动和高效3D机器人手/眼校准的新技术</em></strong> ”，IEEE Trans。抢。自动，5（3），345 –358（1989）。<a href="http://dx.doi.org/10.1109/70.34770">http://dx.doi.org/10.1109/70.34770</a> IRAUEZ 1042-296X <a href="http://scholar.google.com/scholar_lookup?title=A+new+technique+for+fully+autonomous+and+efficient+3D+robotics+hand/eye+calibration&amp;author=R.+Y.+Tsai&amp;author=R.+K.+Lenz&amp;journal=IEEE+Trans.+Rob.+Autom.&amp;volume=5&amp;issue=3&amp;publication_year=1989&amp;pages=345-358">Google学术搜索</a></p>
<p>26 <a id="r26" data-feathr-click-track="true"></a></p>
<p>R. Horaud和F. Dornaika，“ <strong><em>手眼校准</em></strong> ”，诠释。J·罗布 Res。，14（3），195 –210（1995）。<a href="http://dx.doi.org/10.1177/027836499501400301">http://dx.doi.org/10.1177/027836499501400301</a> IJRREL 0278-3649 <a href="http://scholar.google.com/scholar_lookup?title=Hand-eye+calibration&amp;author=R.+Horaud&amp;author=F.+Dornaika&amp;journal=Int.+J.+Rob.+Res.&amp;volume=14&amp;issue=3&amp;publication_year=1995&amp;pages=195-210">Google学术搜索</a></p>
<p>27。 <a id="r27" data-feathr-click-track="true"></a></p>
<p>FC Park和BJ Martin，“ <strong><em>机器人传感器校准：求解Euclidean组上的AX = XB</em></strong> ”，IEEE Trans。抢。自动，10（5），717 –721（1994）。<a href="http://dx.doi.org/10.1109/70.326576">http://dx.doi.org/10.1109/70.326576</a> IRAUEZ 1042-296X <a href="http://scholar.google.com/scholar_lookup?title=Robot+sensor+calibration:+solving+AX+=+XB+on+the+Euclidean+group&amp;author=F.+C.+Park&amp;author=B.+J.+Martin&amp;journal=IEEE+Trans.+Rob.+Autom.&amp;volume=10&amp;issue=5&amp;publication_year=1994&amp;pages=717-721">Google学术搜索</a></p>
<p>28岁 <a id="r28" data-feathr-click-track="true"></a></p>
<p>JCK Chou和M. Kamel，“ <strong><em>使用四元数查找机器人操纵器上的传感器的位置和方向</em></strong> ”，诠释。J·罗布 Res。，10（3），240 –254（1991）。<a href="http://dx.doi.org/10.1177/027836499101000305">http://dx.doi.org/10.1177/027836499101000305</a> IJRREL 0278-3649 <a href="http://scholar.google.com/scholar_lookup?title=Finding+the+position+and+orientation+of+a+sensor+on+a+robot+manipulator+using+quaternions&amp;author=J.+C.+K.+Chou&amp;author=M.+Kamel&amp;journal=Int.+J.+Rob.+Res.&amp;volume=10&amp;issue=3&amp;publication_year=1991&amp;pages=240-254">Google学术搜索</a></p>
<p>29。 <a id="r29" data-feathr-click-track="true"></a></p>
<p>N. Andreff，R。Horaud和B. Espiau，第二**<em>在线中的</em>** “ <strong><em>在线手眼校准</em></strong> ”。Conf。3-D数字成像与建模（目录号PR00062），第430 –436页（1999年）。<a href="http://dx.doi.org/10.1109/IM.1999.805374">http://dx.doi.org/10.1109/IM.1999.805374 </a><a href="http://scholar.google.com/scholar_lookup?title=On-line+hand-eye+calibration&amp;author=N.+Andreff&amp;author=R.+Horaud&amp;author=B.+Espiau&amp;conference=Second+Int.+Conf.+on+3-D+Digital+Imaging+and+Modeling+(Cat.+No.%C2%A0PR00062)&amp;publication_year=1999&amp;pages=430-436">Google学术搜索</a></p>
<p>30岁 <a id="r30" data-feathr-click-track="true"></a></p>
<p>K. Daniilidis，“ <strong><em>使用双四元数进行手眼校准</em></strong> ”，诠释。J·罗布 Res。，18（3），286 –298（1999）。<a href="http://dx.doi.org/10.1177/02783649922066213">http://dx.doi.org/10.1177/02783649922066213</a> IJRREL 0278-3649 <a href="http://scholar.google.com/scholar_lookup?title=Hand-eye+calibration+using+dual+quaternions&amp;author=K.+Daniilidis&amp;journal=Int.+J.+Rob.+Res.&amp;volume=18&amp;issue=3&amp;publication_year=1999&amp;pages=286-298">Google学术搜索</a></p>
<p>31。 <a id="r31" data-feathr-click-track="true"></a></p>
<p>YC Shiu和S. Ahmad，“ <strong><em>通过解决形式为AX = XB的齐次变换方程来校准腕上机器人传感器</em></strong> ”，IEEE Trans。抢。自动，5（1），16 –29（1989）。<a href="http://dx.doi.org/10.1109/70.88014">http://dx.doi.org/10.1109/70.88014</a> IRAUEZ 1042-296X <a href="http://scholar.google.com/scholar_lookup?title=Calibration+of+wrist-mounted+robotic+sensors+by+solving+homogeneous+transform+equations+of+the+form+AX+=+XB&amp;author=Y.+C.+Shiu&amp;author=S.+Ahmad&amp;journal=IEEE+Trans.+Rob.+Autom.&amp;volume=5&amp;issue=1&amp;publication_year=1989&amp;pages=16-29">Google学术搜索</a></p>
<p>32。 <a id="r32" data-feathr-click-track="true"></a></p>
<p>CC Wang，“ <strong><em>安装在机器人上的视觉传感器的外部校准</em></strong> ”，IEEE Trans。抢。自动，8（2），161 –175（1992）。<a href="http://dx.doi.org/10.1109/70.134271">http://dx.doi.org/10.1109/70.134271</a> IRAUEZ 1042-296X <a href="http://scholar.google.com/scholar_lookup?title=Extrinsic+calibration+of+a+vision+sensor+mounted+on+a+robot&amp;author=C.+C.+Wang&amp;journal=IEEE+Trans.+Rob.+Autom.&amp;volume=8&amp;issue=2&amp;publication_year=1992&amp;pages=161-175">Google学术搜索</a></p>
<p>33。 <a id="r33" data-feathr-click-track="true"></a></p>
<p>N. Sariff和N. Buniyamin，“ 第4届学生会议”的“ <strong><em>自主移动机器人路径规划算法概述</em></strong> ”。研究与发展（SCOReD 2006），183 –188（2006）。<a href="http://dx.doi.org/10.1109/SCORED.2006.4339335">http://dx.doi.org/10.1109/SCORED.2006.4339335 </a><a href="http://scholar.google.com/scholar_lookup?title=An+overview+of+autonomous+mobile+robot+path+planning+algorithms&amp;author=N.+Sariff&amp;author=N.+Buniyamin&amp;conference=4th+Student+Conf.+on+Research+and+Development+(SCOReD+2006)&amp;publication_year=2006&amp;pages=183-188">Google学术搜索</a></p>
<p>34。 <a id="r34" data-feathr-click-track="true"></a></p>
<p>M. Reggiani，M。Mazzoli和S. Caselli，“ <strong><em>用于机器人运动计划的碰撞检测程序包的实验评估</em></strong> ”，诠释。Conf。智力 抢。Syst。，3 2329 –2334（2002）。<a href="http://dx.doi.org/10.1109/IRDS.2002.1041615">http://dx.doi.org/10.1109/IRDS.2002.1041615 </a><a href="http://scholar.google.com/scholar_lookup?title=An+experimental+evaluation+of+collision+detection+packages+for+robot+motion+planning&amp;author=M.+Reggiani&amp;author=M.+Mazzoli&amp;author=S.+Caselli&amp;journal=Int.+Conf.+Intell.+Rob.+Syst.&amp;volume=3&amp;publication_year=2002&amp;pages=2329-2334">Google学术搜索</a></p>
<p>35岁 <a id="r35" data-feathr-click-track="true"></a></p>
<p>M. Li，A。Kapoor和RH Taylor，“ IEEE / RSJ诠释中的“ <strong><em>一种针对虚拟夹具的约束优化方法</em></strong> ”。Conf。《智能与机器人系统》，1408–1413年（2005年）。<a href="http://dx.doi.org/10.1109/IROS.2005.1545420">http://dx.doi.org/10.1109/IROS.2005.1545420 </a><a href="http://scholar.google.com/scholar_lookup?title=A+constrained+optimization+approach+to+virtual+fixtures&amp;author=M.+Li&amp;author=A.+Kapoor&amp;author=R.+H.+Taylor&amp;conference=IEEE/RSJ+Int.+Conf.+Intelligent+and+Robotic+Systems&amp;publication_year=2005&amp;pages=1408-1413">Google学术搜索</a></p>
<p>36。 <a id="r36" data-feathr-click-track="true"></a></p>
<p>Y. Otake等人，“ <strong><em>鲁棒的3D–2D图像配准：在存在解剖变形的情况下应用于脊柱干预和椎骨标记</em></strong> ”，物理。中 生物化学杂志，58（23），8535 –8553（2013）。<a href="http://dx.doi.org/10.1088/0031-9155/58/23/8535">http://dx.doi.org/10.1088/0031-9155/58/23/8535</a> PHMBA7 0031-9155 <a href="http://scholar.google.com/scholar_lookup?title=Robust+3D%E2%80%932D+image+registration:+application+to+spine+interventions+and+vertebral+labeling+in+the+presence+of+anatomical+deformation&amp;author=Y.+Otake&amp;journal=Phys.+Med.+Biol.&amp;volume=58&amp;issue=23&amp;publication_year=2013&amp;pages=8535-8553">Google学术搜索</a></p>
<p>37。 <a id="r37" data-feathr-click-track="true"></a></p>
<p>MJD Powell，“ <strong><em>一种无需计算导数即可找到多个变量的最小值的有效方法</em></strong> ”，Comput。J.，7（2），155 –162（1964）。<a href="http://dx.doi.org/10.1093/comjnl/7.2.155">http://dx.doi.org/10.1093/comjnl/7.2.155 </a><a href="http://scholar.google.com/scholar_lookup?title=An+efficient+method+for+finding+the+minimum+of+a+function+of+several+variables+without+calculating+derivatives&amp;author=M.+J.+D.+Powell&amp;journal=Comput.+J.&amp;volume=7&amp;issue=2&amp;publication_year=1964&amp;pages=155-162">Google学术搜索</a></p>
<p>38。 <a id="r38" data-feathr-click-track="true"></a></p>
<p>H. Dang等人，“ <strong><em>在锥束CT介入指导中自动进行图像到世界配准的稳健方法</em></strong> ”，医学杂志。物理学报，39（10），6484 –6498（2012）。<a href="http://dx.doi.org/10.1118/1.4754589">http://dx.doi.org/10.1118/1.4754589</a> MPHYA6 0094-2405 <a href="http://scholar.google.com/scholar_lookup?title=Robust+methods+for+automatic+image-to-world+registration+in+cone-beam+CT+interventional+guidance&amp;author=H.+Dang&amp;journal=Med.+Phys.&amp;volume=39&amp;issue=10&amp;publication_year=2012&amp;pages=6484-6498">Google学术搜索</a></p>
<p>39。 <a id="r39" data-feathr-click-track="true"></a></p>
<p>M. Shoham等人，“ <strong><em>机器人辅助脊柱外科手术-从概念到临床实践</em></strong> ”，计算机。援助外科杂志，12（2），105 –115（2007）。<a href="http://dx.doi.org/10.3109/10929080701243981">http://dx.doi.org/10.3109/10929080701243981 </a><a href="http://scholar.google.com/scholar_lookup?title=Robotic+assisted+spinal+surgery%E2%80%94from+concept+to+clinical+practice&amp;author=M.+Shoham&amp;journal=Comput.+Aided+Surg.&amp;volume=12&amp;issue=2&amp;publication_year=2007&amp;pages=105-115">Google学术搜索</a></p>
<p>40 <a id="r40" data-feathr-click-track="true"></a></p>
<p>NJ Brandmeir等人，“ <strong><em>ROSA立体定向机器人在广泛的临床应用和注册技术中的相对准确性</em></strong> ” <strong><em>，</em></strong> J。Rob 。外科。1-7（2017）。<a href="http://dx.doi.org/10.1007/s11701-017-0712-2">http://dx.doi.org/10.1007/s11701-017-0712-2 </a><a href="http://scholar.google.com/scholar_lookup?title=The+comparative+accuracy+of+the+ROSA+stereotactic+robot+across+a+wide+range+of+clinical+applications+and+registration+techniques&amp;author=N.+J.+Brandmeir&amp;journal=J.+Rob.+Surg.&amp;publication_year=2017&amp;pages=1-7">Google学术搜索</a></p>
<h2 id="传">传</h2>
<p><a id="b1" data-feathr-click-track="true"></a></p>
<p><strong>Thomas Yi</strong>是约翰霍普金斯医学院的研究助理。他于2017年获得约翰·霍普金斯大学生物医学工程和计算机科学学士学位。他的研究兴趣包括图像引导手术和外科机器人。</p>
<p><a id="b2" data-feathr-click-track="true"></a></p>
<p><strong>Vignesh Ramchandran</strong>是约翰·霍普金斯大学应用数学和统计学的硕士研究生。他于2017年获得约翰霍普金斯大学生物医学工程学士学位。他的研究兴趣包括机器学习，压缩感测和医学图像分析。</p>
<p><a id="b3" data-feathr-click-track="true"></a></p>
<p><strong>Jeffrey H. Siewerdsen</strong>于1998年在密歇根大学获得物理学博士学位。他是生物医学工程，计算机科学，放射学和神经外科的教授，也是约翰·霍普金斯大学卡内基外科创新中心的主任。他的研究专注于数字X射线，CT和锥束CT图像质量的物理学，以及系统开发，以提高图像引导干预的准确性，准确性和安全性。</p>
<p><a id="b4" data-feathr-click-track="true"></a></p>
<p><strong>Ali Uneri</strong>于2017年获得约翰·霍普金斯大学的计算机科学博士学位。他是约翰·霍普金斯大学生物医学工程系的博士后。他的研究专注于图像配准，3D图像重建以及用于图像引导干预的外科手术机器人。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[elestrix]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/xnapG7-CO</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/xnapG7-CO">
        </link>
        <updated>2020-06-22T15:51:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="par0013">Par0013</h1>
<p>|</p>
<h2 id="内容">内容</h2>
<p>[ <a href="http://elastix.bigr.nl/wiki/index.php/Par0013">隐藏</a> ]</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Image_data">1 图像数据</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Application">2 申请</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Registration_settings">3 注册设置</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Published_in">4 发表于</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#Other_comments">5 其他评论</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Par0013#References">6 参考</a></li>
</ul>
<p>|</p>
<h3 id="影像数据">影像数据</h3>
<ul>
<li>3D锥形束CT和2D X射线</li>
<li>神经介入性头/颈</li>
<li>X射线像素尺寸：0.86 x 0.86 x 1.0毫米; 尺寸：256 x 256 x 1</li>
</ul>
<p>（注意：2D图像应作为3D尺寸为1的3D图像给出）。</p>
<ul>
<li>CBCT体素尺寸：0.58 x 0.58 x 0.58 mm; 尺寸：256 x 256 x 256</li>
<li>使用X射线血管造影C型臂系统（Allura Xper FD20，Philips Healthcare，Best，荷兰）获得。</li>
<li>以MHD格式存储。</li>
<li>数据来源于马萨诸塞州伍斯特市麻省大学医学院放射学系。</li>
</ul>
<p>示例数据：</p>
<p><a href="http://elastix.bigr.nl/wiki/images/a/ae/ExampleData.zip" title="ExampleData.zip">媒体：ExampleData.zip</a></p>
<p>屏幕截图：</p>
<p><a href="http://elastix.bigr.nl/wiki/index.php/File:Screenshot_x-ray.jpg"><img src="http://elastix.bigr.nl/wiki/images/thumb/e/e3/Screenshot_x-ray.jpg/316px-Screenshot_x-ray.jpg" alt="截图x-ray.jpg"></a><a href="http://elastix.bigr.nl/wiki/index.php/File:Screenshot_x-ray_lat.jpg"><img src="http://elastix.bigr.nl/wiki/images/c/c3/Screenshot_x-ray_lat.jpg" alt="截图X射线lat.jpg"></a></p>
<h3 id="应用">应用</h3>
<p>比较了7种优化方法的组合的2D-3D注册性能：</p>
<p>*定期逐步下降
*内尔德米德
*鲍威尔·布伦特
*拟牛顿
*非线性共轭梯度
*同时摄动随机近似
*进化策略</p>
<p>和三个相似性度量：</p>
<p>*梯度差异
*归一化梯度相关
*图案强度。</p>
<p>相似性度量已添加到<tt>elastix中，</tt>并且仅<strong>用于2D-3D</strong>。</p>
<p>对在脑干预过程中获得的患者数据集进行了实验。评估了各种组分组合的配准精度，捕获范围和配准时间。结果表明，对于相同的相似性度量，使用不同的优化方法可获得不同的配准精度和捕获范围。总体而言，可以得出结论，就准确性，捕获范围和计算时间而言，Powell-Brent是将X射线图像基于强度的2D-3D注册到CBCT的可靠优化方法。</p>
<h3 id="注册设置">注册设置</h3>
<p><tt>elastix</tt>版本：4.5</p>
<p>使用单个X射线图像的参数文件：</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/images/3/3f/Par0013Powell_GD_singleImage.txt" title="Par0013鲍威尔GD singleImage.txt">媒体：par0013Powell_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/28/Par0013Powell_NGC_singleImage.txt" title="Par0013鲍威尔NGC singleImage.txt">媒体：par0013Powell_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/2c/Par0013Powell_PI_singleImage.txt" title="Par0013鲍威尔PI singleImage.txt">媒体：par0013Powell_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/0/04/Par0013NM_GD_singleImage.txt" title="Par0013NM GD singleImage.txt">媒体：par0013NM_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/19/Par0013NM_NGC_singleImage.txt" title="Par0013NM NGC singleImage.txt">媒体：par0013NM_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/7/76/Par0013NM_PI_singleImage.txt" title="Par0013NM PI singleImage.txt">媒体：par0013NM_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/9/94/Par0013GC_GD_singleImage.txt" title="Par0013GC GD singleImage.txt">媒体：par0013GC_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/16/Par0013GC_NGC_singleImage.txt" title="Par0013GC NGC singleImage.txt">媒体：par0013GC_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/f/f3/Par0013GC_PI_singleImage.txt" title="Par0013GC PI singleImage.txt">媒体：par0013GC_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/e/e4/Par0013QN_GD_singleImage.txt" title="Par0013QN GD singleImage.txt">媒体：par0013QN_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/6/60/Par0013QN_NGC_singleImage.txt" title="Par0013QN NGC singleImage.txt">媒体：par0013QN_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/4/4a/Par0013QN_PI_singleImage.txt" title="Par0013QN PI singleImage.txt">媒体：par0013QN_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/2/2d/Par0013RSGD_GD_singleImage.txt" title="Par0013RSGD GD singleImage.txt">媒体：par0013RSGD_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/6/69/Par0013RSGD_NGC_singleImage.txt" title="Par0013RSGD NGC singleImage.txt">媒体：par0013RSGD_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/0/0f/Par0013RSGD_PI_singleImage.txt" title="Par0013RSGD PI singleImage.txt">媒体：par0013RSGD_PI_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/1/1d/Par0013CMAES_GD_singleImage.txt" title="Par0013CMAES GD singleImage.txt">媒体：par0013CMAES_GD_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/3/36/Par0013CMAES_NGC_singleImage.txt" title="Par0013CMAES NGC singleImage.txt">媒体：par0013CMAES_NGC_singleImage.txt</a></li>
<li><a href="http://elastix.bigr.nl/wiki/images/e/e3/Par0013CMAES_PI_singleImage.txt" title="Par0013CMAES PI singleImage.txt">媒体：par0013CMAES_PI_singleImage.txt</a></li>
</ul>
<p>使用单个X射线图像的命令行调用：</p>
<p>elastix -f xrayImage.mhd -fMask xrayMask.mhd -m volumeData.mhd -p par0013Powel_NGC_singleImage.txt -out outputdir</p>
<p>使用多个X射线图像的参数文件（注意：示例显示2，但是可以接受更多固定图像）：</p>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/images/9/9d/Par0013Powell_NGC_twoImages.txt" title="Par0013鲍威尔NGC twoImages.txt">媒体：par0013Powell_NGC_twoImages.txt</a></li>
</ul>
<p>使用两个X射线图像进行命令行调用：</p>
<p>elastix -f0 xrayImage1.mhd -f1 xrayImage2.mhd -f0Mask xrayMask1.mhd -f1Mask xrayMask1.mhd -m volumeData.mhd -p par0013Powell_NGC_twoImages.txt -out outputdir</p>
<h3 id="出版于">出版于</h3>
<p>这些注册在出版物中描述：</p>
<p>IMJ van der Bom，S。Klein，M。Staring，R。Homan，LW Bartels，JPW Pluim，“在X射线引导的干预措施中基于强度的2D-3D配准的优化方法的评估”，载于：SPIE医学影像：图像处理，SPIE出版社，第一卷。7962，第796223-1-796223-15，2011年。</p>
<h3 id="其他的建议">其他的建议</h3>
<h3 id="参考资料">参考资料</h3>
<p>[1] IMJ van der Bom，S。Klein，M。Staring，R。Homan，LW Bartels，JPW Pluim，“在X射线引导的干预措施中基于强度的2D-3D配准的优化方法的评估”，载于：SPIE医学成像：图像处理，SPIE出版社，第1卷。7962，第796223-1-796223-15，2011年。</p>
<ul>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/Par0013" title="查看内容页面[alt-c]">页</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Talk:Par0013&amp;action=edit&amp;redlink=1" title="关于内容页面的讨论[alt-t]">讨论区</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;action=edit" title="此页面受保护。 您可以查看其来源[alt-e]">查看资料</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;action=history" title="此页面的先前修订版[alt-h]">历史</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php?title=Special:UserLogin&amp;returnto=Par0013" title="鼓励您登录； 但是，它不是强制性的[alt-o]">登录/创建账户</a></p>
</li>
</ul>
<h5 id="导航">导航</h5>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Main_Page" title="访问主页[alt-z]">主页</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Community_portal" title="关于项目，您可以做什么，在哪里找到东西">社区门户</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Current_events" title="查找有关当前事件的背景信息">现在发生的事</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:RecentChanges" title="Wiki [alt-r]中最近更改的列表">近期变动</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:Random" title="加载随机页面[alt-x]">随机页面</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Help:Contents" title="寻找的地方">救命</a></li>
</ul>
<h5 id="搜索">搜索</h5>
<h5 id="工具箱">工具箱</h5>
<ul>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:WhatLinksHere/Par0013" title="在此处链接的所有维基页面的列表[alt-j]">这里有什么链接</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:RecentChangesLinked/Par0013" title="与此页面链接的页面的最新更改[alt-k]">相关变更</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php/Special:SpecialPages" title="所有特殊页面的列表[alt-q]">特殊页面</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;printable=yes">可打印的版本</a></li>
<li><a href="http://elastix.bigr.nl/wiki/index.php?title=Par0013&amp;oldid=981" title="永久链接到此页面的修订版">永久链接</a></li>
</ul>
<p><a href="http://www.gnu.org/licenses/old-licenses/fdl-1.2.txt"><img src="http://elastix.bigr.nl/wiki/skins/common/images/gnu-fdl.png" alt="GNU自由文档许可证1.2"></a></p>
<p><a href="http://www.mediawiki.org/"><img src="http://elastix.bigr.nl/wiki/skins/common/images/poweredby_mediawiki_88x31.png" alt="由MediaWiki提供支持"></a></p>
<ul>
<li>
<p>该页面的最后修改时间为2012年2月6日12:50。</p>
</li>
<li>
<p>本页面已经被浏览37,219次。</p>
</li>
<li>
<p>内容可在<a href="http://www.gnu.org/licenses/old-licenses/fdl-1.2.txt">GNU Free Documentation License 1.2下获得</a>。</p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:Privacy_policy" title="elastix：隐私政策">隐私政策</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:About" title="elastix：关于">关于elastix</a></p>
</li>
<li>
<p><a href="http://elastix.bigr.nl/wiki/index.php/elastix:General_disclaimer" title="elastix：一般免责声明">免责声明</a></p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何利用AI创建一个空中橡皮擦]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/y0oFym3ys</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/y0oFym3ys">
        </link>
        <updated>2020-06-22T15:05:44.000Z</updated>
        <content type="html"><![CDATA[<p>如何利用AI创建一个空中橡皮擦
如果您可以在空中挥动笔以虚拟地绘制东西，然后实际上将其绘制在屏幕上，那会不会很酷？如果我们不使用任何特殊的硬件来实际实现这一目标，那可能会更加有趣，只需简单的计算机视觉即可，实际上，我们甚至不需要使用机器学习或深度学习来实现这一目标。</p>
<p>这是我们将构建的应用程序的演示。</p>
<p>因此，在本文中，您将学习如何创建自己的虚拟笔和虚拟橡皮擦。整个应用程序将从根本上构建轮廓检测。您可以将Contours看作是一条闭合的曲线，具有相同的颜色或强度，就像一个斑点，您可以<a href="https://docs.opencv.org/4.2.0/d4/d73/tutorial_py_contours_begin.html">在此处</a>阅读有关轮廓的更多信息  。</p>
<h2 id="它是如何工作的">它是如何工作的：</h2>
<p>因此，这是我们将如何实现此目的的方法，首先，我们将使用颜色遮罩来获得目标彩色笔的二进制遮罩（（<em>我将使用蓝色标记作为虚拟笔</em>）），然后使用轮廓检测​​来在整个屏幕上检测并跟踪该笔的位置。</p>
<p>一旦完成，只需按_字面_连接点  ，是的，您只需要使用笔的先前位置（<em>在前一帧中的位置</em>）的x，y坐标与新的x，y点（<em>x，y指向新框架</em>），就是这样，您有一支虚拟笔。</p>
<h2 id="结构体">结构体：</h2>
<p>当然，现在要进行预处理，并添加一些其他功能，因此这里是我们应用程序每个步骤的细分。</p>
<ol>
<li><strong>步骤1：</strong> <em>找到目标对象的颜色范围并保存。</em></li>
<li><strong>第2步：</strong> <em>应用正确的形态学操作以减少视频中的噪声</em></li>
<li><strong>步骤3：</strong> <em>通过轮廓检测来检测和跟踪有色物体。</em></li>
<li><strong>第4步：</strong> <em>找到要在屏幕上绘制的对象的x，y位置坐标。</em></li>
<li><em>**步骤5：**添加雨刮器功能以擦除整个屏幕。</em></li>
<li><em>**步骤6：**添加橡皮擦功能以擦除图形的某些部分。</em></li>
</ol>
<p>我已经设计了此应用程序的管道，以便可以轻松地将其重新用于其他项目，例如，如果要制作任何涉及跟踪彩色对象的项目，则可以使用步骤1-3。同样，这种故障使您自己运行一个步骤时调试起来变得容易得多，因为您将确切知道错误的步骤。每个步骤都可以独立运行。</p>
<p>请注意，我们在第4步已准备好虚拟笔，因此我在第5-6步中添加了更多功能，例如，在第5步中，有一个虚拟刮水器，可以像从屏幕上擦除笔一样从屏幕上擦除笔标记。然后在第6步中，我们将添加一个切换器，使您可以使用橡皮擦切换笔。因此，让我们开始吧。</p>
<p>首先导入所需的库</p>
<pre><code>import cv2
import numpy as np
import time

</code></pre>
<h3 id="步骤1找到目标笔的颜色范围并保存">步骤1：找到目标笔的颜色范围并保存</h3>
<p>首先，我们必须为目标彩色对象找到合适的颜色范围，该范围将在  <code>cv2.inrange()</code> 功能上用于过滤掉我们的对象。我们还将范围数组另存为<code>.npy</code>文件到磁盘中，以便以后访问。</p>
<p>由于我们正在尝试进行颜色检测，因此我们将RGB（或OpenCV中的BGR）格式的图像转换为HSV（色相，饱和度，值）颜色格式，因为它在该模型中更容易处理颜色。</p>
<p>下面的脚本将使您可以使用轨迹栏来调整图像的色相，饱和度和值通道。调整轨迹栏，直到只有您的目标对象可见，其余为黑色。</p>
<pre><code># A required callback method that goes into the trackbar function.
def nothing(x):
    pass

# Initializing the webcam feed.
cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# Create a window named trackbars.
cv2.namedWindow(&quot;Trackbars&quot;)

# Now create 6 trackbars that will control the lower and upper range of 
# H,S and V channels. The Arguments are like this: Name of trackbar, 
# window name, range,callback function. For Hue the range is 0-179 and
# for S,V its 0-255.
cv2.createTrackbar(&quot;L - H&quot;, &quot;Trackbars&quot;, 0, 179, nothing)
cv2.createTrackbar(&quot;L - S&quot;, &quot;Trackbars&quot;, 0, 255, nothing)
cv2.createTrackbar(&quot;L - V&quot;, &quot;Trackbars&quot;, 0, 255, nothing)
cv2.createTrackbar(&quot;U - H&quot;, &quot;Trackbars&quot;, 179, 179, nothing)
cv2.createTrackbar(&quot;U - S&quot;, &quot;Trackbars&quot;, 255, 255, nothing)
cv2.createTrackbar(&quot;U - V&quot;, &quot;Trackbars&quot;, 255, 255, nothing)
 
 
while True:
    
    # Start reading the webcam feed frame by frame.
    ret, frame = cap.read()
    if not ret:
        break
    # Flip the frame horizontally (Not required)
    frame = cv2.flip( frame, 1 ) 
    
    # Convert the BGR image to HSV image.
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # Get the new values of the trackbar in real time as the user changes 
    # them
    l_h = cv2.getTrackbarPos(&quot;L - H&quot;, &quot;Trackbars&quot;)
    l_s = cv2.getTrackbarPos(&quot;L - S&quot;, &quot;Trackbars&quot;)
    l_v = cv2.getTrackbarPos(&quot;L - V&quot;, &quot;Trackbars&quot;)
    u_h = cv2.getTrackbarPos(&quot;U - H&quot;, &quot;Trackbars&quot;)
    u_s = cv2.getTrackbarPos(&quot;U - S&quot;, &quot;Trackbars&quot;)
    u_v = cv2.getTrackbarPos(&quot;U - V&quot;, &quot;Trackbars&quot;)
 
    # Set the lower and upper HSV range according to the value selected
    # by the trackbar
    lower_range = np.array([l_h, l_s, l_v])
    upper_range = np.array([u_h, u_s, u_v])
    
    # Filter the image and get the binary mask, where white represents 
    # your target color
    mask = cv2.inRange(hsv, lower_range, upper_range)
 
    # You can also visualize the real part of the target color (Optional)
    res = cv2.bitwise_and(frame, frame, mask=mask)
    
    # Converting the binary mask to 3 channel image, this is just so 
    # we can stack it with the others
    mask_3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
    
    # stack the mask, orginal frame and the filtered result
    stacked = np.hstack((mask_3,frame,res))
    
    # Show this stacked frame at 40% of the size.
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.4,fy=0.4))
    
    # If the user presses ESC then exit the program
    key = cv2.waitKey(1)
    if key == 27:
        break
    
    # If the user presses `s` then print this array.
    if key == ord('s'):
        
        thearray = [[l_h,l_s,l_v],[u_h, u_s, u_v]]
        print(thearray)
        
        # Also save this array as penval.npy
        np.save('penval',thearray)
        break
    
# Release the camera &amp; destroy the windows.    
cap.release()
cv2.destroyAllWindows()
</code></pre>
<h3 id="步骤2最大化检测掩模并消除噪声">步骤2：最大化检测掩模并消除噪声</h3>
<p>现在，不需要在上一步中获得完美的蒙版，也可以在图像中出现一些像白点这样的杂色，我们可以在此步骤中通过形态学操作消除这种杂色。</p>
<p>现在，我使用一个名为<strong>load_from_disk</strong>的变量   来确定是要从磁盘加载颜色范围还是要使用一些自定义值。</p>
<pre><code># This variable determines if we want to load color range from memory 
# or use the ones defined here. 
load_from_disk = True

# If true then load color range from memory
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# Creating A 5x5 kernel for morphological operations
kernel = np.ones((5,5),np.uint8)

while(1):
    
    ret, frame = cap.read()
    if not ret:
        break
        
    frame = cv2.flip( frame, 1 )

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise.
    # Erosion Eats away the white part while dilation expands it.
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)

    res = cv2.bitwise_and(frame,frame, mask= mask)

    mask_3 = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)
    
    # stack all frames and show it
    stacked = np.hstack((mask_3,frame,res))
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.4,fy=0.4))
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤3追踪目标笔">步骤3：追踪目标笔</h3>
<p>现在我们有了一个不错的蒙版，我们可以使用它通过轮廓检测来检测笔。我们将在对象周围绘制一个边界框，以确保在整个屏幕上都可以检测到它。</p>
<pre><code># This variable determines if we want to load color range from memory 
# or use the ones defined in the notebook. 
load_from_disk = True

# If true then load color range from memory
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

# kernel for morphological operations
kernel = np.ones((5,5),np.uint8)

# set the window to auto-size so we can view this full screen.
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This threshold is used to filter noise, the contour area must be 
# bigger than this to qualify as an actual contour.
noiseth = 500

while(1):
    
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower 
    # ranges from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours in the frame.
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL,
                                           cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also make sure its size 
    # is bigger than noise threshold.
    if contours and cv2.contourArea(max(contours, 
                               key = cv2.contourArea)) &gt; noiseth:
        
        # Grab the biggest contour with respect to area
        c = max(contours, key = cv2.contourArea)
        
        # Get bounding box coordinates around that contour
        x,y,w,h = cv2.boundingRect(c)
        
        # Draw that bounding box
        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,25,255),2)        

    cv2.imshow('image',frame)
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤4使用笔绘图">步骤4：使用笔绘图</h3>
<p>现在，所有内容都已设置好，我们可以轻松跟踪目标对象了，现在可以使用该对象在屏幕上进行虚拟绘制了。</p>
<p>现在我们只需要做的是使用  <code>x</code>，<code>y</code> 位置，距离返回  <strong><code>cv2.boundingRect()</code></strong> 功能从以前的帧（F-1），并将其与连接  <code>x</code>，<code>y</code> 物体的坐标，在新帧（F）。通过连接这两个点，我们绘制了一条线，并针对网络摄像头提要中的每一帧进行了绘制，通过这种方式，我们将看到使用笔进行的实时绘制。</p>
<p>**注意：**我们将在黑色画布上绘制，然后将该画布与框架合并。这是因为每次迭代都会获得一个新的框架，因此我们无法使用实际的框架。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

kernel = np.ones((5,5),np.uint8)

# Initializing the canvas on which we will draw upon
canvas = None

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initialize the canvas as a black image of the same size as the frame.
    if canvas is None:
        canvas = np.zeros_like(frame)

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours
    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also its size is bigger than 
    # the noise threshold.
    if contours and cv2.contourArea(max(contours, 
                                 key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        # This is true when we writing for the first time or when writing 
        # again when the pen had disappeared from view.
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            # Draw the line on the canvas
            canvas = cv2.line(canvas, (x1,y1),(x2,y2), [255,0,0], 4)
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
    # Merge the canvas and the frame.
    frame = cv2.add(frame,canvas)
    
    # Optionally stack both frames and show it.
    stacked = np.hstack((canvas,frame))
    cv2.imshow('Trackbars',cv2.resize(stacked,None,fx=0.6,fy=0.6))

    k = cv2.waitKey(1) &amp; 0xFF
    if k == 27:
        break
        
    # When c is pressed clear the canvas
    if k == ord('c'):
        canvas = None

cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤5添加图像刮水器">步骤5：添加图像刮水器</h3>
<p>在上面的脚本中，我们有一支可以正常工作的虚拟笔，并且当用户按下<code>c</code> 按钮时我们也清理或擦拭了屏幕  ，现在让我们也自动执行此擦拭部件的工作。一种简单的方法是检测目标对象何时离摄像机太近，然后如果目标离摄像机太近，则清除屏幕。</p>
<p>轮廓的大小随着它靠近相机而增加，因此我们可以监控轮廓的大小以实现此目的。</p>
<p>我们要做的另一件事是，我们还将警告用户我们将在几秒钟内清除屏幕，以便他/她可以将物体从框架中取出。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)
cap.set(3,1280)
cap.set(4,720)

kernel = np.ones((5,5),np.uint8)

# Making window size adjustable
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This is the canvas on which we will draw upon
canvas=None

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

# Threshold for wiper, the size of the contour must be bigger than for us to
# clear the canvas 
wiper_thresh = 40000

# A variable which tells when to clear canvas, if its True then we clear the canvas
clear = False

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initialize the canvas as a black image
    if canvas is None:
        canvas = np.zeros_like(frame)

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform the morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours.
    contours, hierarchy = cv2.findContours(mask,
    cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also its size is bigger than 
    # the noise threshold.
    if contours and cv2.contourArea(max(contours,
                                   key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # Get the area of the contour
        area = cv2.contourArea(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            # Draw the line on the canvas
            canvas = cv2.line(canvas, (x1,y1),(x2,y2),
            [255,0,0], 5)
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2
        
        # Now if the area is greater than the wiper threshold then set the  
        # clear variable to True and warn User.
        if area &gt; wiper_thresh:
           cv2.putText(canvas,'Clearing Canvas', (100,200), 
           cv2.FONT_HERSHEY_SIMPLEX,2, (0,0,255), 5, cv2.LINE_AA)
           clear = True 

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
   
    # Now this piece of code is just for smooth drawing. (Optional)
    _ , mask = cv2.threshold(cv2.cvtColor(canvas, cv2.COLOR_BGR2GRAY), 20, 
    255, cv2.THRESH_BINARY)
    foreground = cv2.bitwise_and(canvas, canvas, mask = mask)
    background = cv2.bitwise_and(frame, frame,
    mask = cv2.bitwise_not(mask))
    frame = cv2.add(foreground,background)

    cv2.imshow('image',frame)
    
    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break
    
    # Clear the canvas after 1 second if the clear variable is true
    if clear == True:
        
        time.sleep(1)
        canvas = None
        
        # And then set clear to false
        clear = False
        
cv2.destroyAllWindows()
cap.release()
</code></pre>
<h3 id="步骤6添加橡皮擦功能">步骤6：添加橡皮擦功能</h3>
<p>既然我们已经完成了笔和刮水器的工作，接下来就可以添加橡皮擦功能了。所以我想要的就是这样，当用户切换到橡皮擦而不是绘画时，它会擦除​​笔画的那一部分。做到这一点真的很容易，您只需要在画布上用黑色绘制橡皮擦就可以了。通过涂成黑色，该部分在合并期间恢复为原始状态，因此就像橡皮擦一样。橡皮擦功能的真正编码部分是如何在笔和橡皮擦之间进行切换，当然，最简单的方法是使用键盘按钮，但是我们想要比这更酷的东西。</p>
<p>因此，我们要做的就是每当有人将手放在屏幕的左上角时执行切换。我们将使用背景减法来监视该区域，以便我们知道何时会有干扰。就像按下虚拟按钮一样。</p>
<pre><code>load_from_disk = True
if load_from_disk:
    penval = np.load('penval.npy')

cap = cv2.VideoCapture(0)

# Load these 2 images and resize them to the same size.
pen_img = cv2.resize(cv2.imread('pen.png',1), (50, 50))
eraser_img = cv2.resize(cv2.imread('eraser.jpg',1), (50, 50))

kernel = np.ones((5,5),np.uint8)

# Making window size adjustable
cv2.namedWindow('image', cv2.WINDOW_NORMAL)

# This is the canvas on which we will draw upon
canvas = None

# Create a background subtractor Object
backgroundobject = cv2.createBackgroundSubtractorMOG2(detectShadows = False)

# This threshold determines the amount of disruption in the background.
background_threshold = 600

# A variable which tells you if you're using a pen or an eraser.
switch = 'Pen'

# With this variable we will monitor the time between previous switch.
last_switch = time.time()

# Initilize x1,y1 points
x1,y1=0,0

# Threshold for noise
noiseth = 800

# Threshold for wiper, the size of the contour must be bigger than this for # us to clear the canvas
wiper_thresh = 40000

# A variable which tells when to clear canvas
clear = False

while(1):
    _, frame = cap.read()
    frame = cv2.flip( frame, 1 )
    
    # Initilize the canvas as a black image
    if canvas is None:
        canvas = np.zeros_like(frame)
        
    # Take the top left of the frame and apply the background subtractor
    # there    
    top_left = frame[0: 50, 0: 50]
    fgmask = backgroundobject.apply(top_left)
    
    # Note the number of pixels that are white, this is the level of 
    # disruption.
    switch_thresh = np.sum(fgmask==255)
    
    # If the disruption is greater than background threshold and there has 
    # been some time after the previous switch then you. can change the 
    # object type.
    if switch_thresh&gt;background_threshold and (time.time()-last_switch) &gt; 1:

        # Save the time of the switch. 
        last_switch = time.time()
        
        if switch == 'Pen':
            switch = 'Eraser'
        else:
            switch = 'Pen'

    # Convert BGR to HSV
    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
    
    # If you're reading from memory then load the upper and lower ranges 
    # from there
    if load_from_disk:
            lower_range = penval[0]
            upper_range = penval[1]
            
    # Otherwise define your own custom values for upper and lower range.
    else:             
       lower_range  = np.array([26,80,147])
       upper_range = np.array([81,255,255])
    
    mask = cv2.inRange(hsv, lower_range, upper_range)
    
    # Perform morphological operations to get rid of the noise
    mask = cv2.erode(mask,kernel,iterations = 1)
    mask = cv2.dilate(mask,kernel,iterations = 2)
    
    # Find Contours
    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, 
    cv2.CHAIN_APPROX_SIMPLE)
    
    # Make sure there is a contour present and also it size is bigger than 
    # noise threshold.
    if contours and cv2.contourArea(max(contours,
                                      key = cv2.contourArea)) &gt; noiseth:
                
        c = max(contours, key = cv2.contourArea)    
        x2,y2,w,h = cv2.boundingRect(c)
        
        # Get the area of the contour
        area = cv2.contourArea(c)
        
        # If there were no previous points then save the detected x2,y2 
        # coordinates as x1,y1. 
        if x1 == 0 and y1 == 0:
            x1,y1= x2,y2
            
        else:
            if switch == 'Pen':
                # Draw the line on the canvas
                canvas = cv2.line(canvas, (x1,y1),
                (x2,y2), [255,0,0], 5)
                
            else:
                cv2.circle(canvas, (x2, y2), 20,
                (0,0,0), -1)
            
            
        
        # After the line is drawn the new points become the previous points.
        x1,y1= x2,y2
        
        # Now if the area is greater than the wiper threshold then set the 
        # clear variable to True
        if area &gt; wiper_thresh:
           cv2.putText(canvas,'Clearing Canvas',(0,200), 
           cv2.FONT_HERSHEY_SIMPLEX, 2, (0,0,255), 1, cv2.LINE_AA)
           clear = True 

    else:
        # If there were no contours detected then make x1,y1 = 0
        x1,y1 =0,0
    
   
    # Now this piece of code is just for smooth drawing. (Optional)
    _ , mask = cv2.threshold(cv2.cvtColor (canvas, cv2.COLOR_BGR2GRAY), 20, 
    255, cv2.THRESH_BINARY)
    foreground = cv2.bitwise_and(canvas, canvas, mask = mask)
    background = cv2.bitwise_and(frame, frame,
    mask = cv2.bitwise_not(mask))
    frame = cv2.add(foreground,background)

    # Switch the images depending upon what we're using, pen or eraser.
    if switch != 'Pen':
        cv2.circle(frame, (x1, y1), 20, (255,255,255), -1)
        frame[0: 50, 0: 50] = eraser_img
    else:
        frame[0: 50, 0: 50] = pen_img

    cv2.imshow('image',frame)

    k = cv2.waitKey(5) &amp; 0xFF
    if k == 27:
        break
    
    # Clear the canvas after 1 second, if the clear variable is true
    if clear == True: 
        time.sleep(1)
        canvas = None
        
        # And then set clear to false
        clear = False
        
cv2.destroyAllWindows()
cap.release()
</code></pre>
<p>注意：  我选择的所有这些不同阈值的值都将取决于您的环境，因此请先对其进行调整，而不要尝试使我的值起作用。</p>
<p>我用一张白纸盖住了蓝色标记，除了一张纸以外，所有面都用白纸盖住了，这样我就可以避免连续画图，并且在画图之间留有缝隙。
同样，对于需要通过颜色检测对象的任何应用程序，您可以重复使用步骤1-3。我希望你们中的一些人尝试扩展此应用程序，并可能构建更酷的功能。</p>
<p>希望您喜欢本教程，谢谢。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[ITK教程 python]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/m-GVHor_Q</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/m-GVHor_Q">
        </link>
        <updated>2020-06-12T05:37:32.000Z</updated>
        <content type="html"><![CDATA[<h1 id="打开一个图片并且标准转化保存图片">打开一个图片并且标准转化保存图片</h1>
<pre><code>import itk
import sys
import vtk

inputfilename=&quot;D:/DATA/small/0200.dcm&quot;
outputfilename=&quot;D:/DATA/0200.png&quot;
#
# Reads a 2D image in with signed short (16bits/pixel) pixel type
# and save it as unsigned char (8bits/pixel) pixel type
#
InputImageType  = itk.Image.SS2
OutputImageType = itk.Image.UC2

reader = itk.ImageFileReader[InputImageType].New()
writer = itk.ImageFileWriter[OutputImageType].New()


filter = itk.RescaleIntensityImageFilter[InputImageType, OutputImageType].New()
filter.SetOutputMinimum( 0 )
filter.SetOutputMaximum(255)

filter.SetInput( reader.GetOutput() )
writer.SetInput( filter.GetOutput() )

reader.SetFileName(inputfilename )
writer.SetFileName(outputfilename )

writer.Update()
</code></pre>
<h1 id="itk打开后vtk查看">Itk打开后VTK查看</h1>
<p>在上述代码中加入</p>
<pre><code>ivfilter=itk.ImageToVTKImageFilter[OutputImageType].New()
ivfilter.SetInput(filter.GetOutput())
viewer=vtk.vtkImageViewer()
iren=vtk.vtkRenderWindowInteractor()
viewer.SetupInteractor(iren)
viewer.SetInput(ivfilter.GetOutput())
viewer.Render()
viewer.SetColorWindow(255)
viewer.SetColorLevel(128)
iren.Start()
</code></pre>
<h1 id="itk-配准的例子">ITK 配准的例子</h1>
<pre><code>from InsightToolkit import *
from sys import argv

fixedImageName=&quot;D:/DATA/Brain.png&quot;
movingImageName=&quot;D:/DATA/Brainshifted13x17y.png&quot;
outputImageName=&quot;D:/DATA/Brainresult2.png&quot;

fixedImageReader  = itkImageFileReaderIF2_New()
movingImageReader = itkImageFileReaderIF2_New()

fixedImageReader.SetFileName(  fixedImageName )
movingImageReader.SetFileName( movingImageName )

fixedImageReader.Update()
movingImageReader.Update()

fixedImage  = fixedImageReader.GetOutput()
movingImage = movingImageReader.GetOutput()

#
#  Instantiate the classes for the registration framework
#
registration = itkImageRegistrationMethodIF2IF2_New()
imageMetric  = itkMeanSquaresImageToImageMetricIF2IF2_New()
transform    = itkTranslationTransformD2_New()
optimizer    = itkRegularStepGradientDescentOptimizer_New()
interpolator = itkLinearInterpolateImageFunctionIF2D_New()


registration.SetOptimizer(    optimizer.GetPointer()    )
registration.SetTransform(    transform.GetPointer()    )
registration.SetInterpolator( interpolator.GetPointer() )
registration.SetMetric(       imageMetric.GetPointer()  )
registration.SetFixedImage(  fixedImage  )
registration.SetMovingImage( movingImage )

registration.SetFixedImageRegion( fixedImage.GetBufferedRegion() )

transform.SetIdentity()
initialParameters = transform.GetParameters()

registration.SetInitialTransformParameters( initialParameters )

#
# Iteration Observer
#
def iterationUpdate():
    currentParameter = transform.GetParameters()
    print &quot;M: %f   P: %f %f &quot; % ( optimizer.GetValue(),
                        currentParameter.GetElement(0),
                        currentParameter.GetElement(1) )

iterationCommand = itkPyCommand_New()
iterationCommand.SetCommandCallable( iterationUpdate )
optimizer.AddObserver( itkIterationEvent(), iterationCommand.GetPointer() )



#
#  Define optimizer parameters
#
optimizer.SetMaximumStepLength(  4.00 )
optimizer.SetMinimumStepLength(  0.01 )
optimizer.SetNumberOfIterations( 200  )


print &quot;Starting registration&quot;

#
#  Start the registration process
#

registration.Update()


#
# Get the final parameters of the transformation
#
finalParameters = registration.GetLastTransformParameters()

print &quot;Final Registration Parameters &quot;
print &quot;Translation X =  %f&quot; % (finalParameters.GetElement(0),)
print &quot;Translation Y =  %f&quot; % (finalParameters.GetElement(1),)

#
# Now, we use the final transform for resampling the
# moving image.
#
resampler = itkResampleImageFilterIF2IF2_New()
resampler.SetTransform( transform.GetPointer()    )
resampler.SetInput(     movingImage  )

region = fixedImage.GetLargestPossibleRegion()

resampler.SetSize( region.GetSize() )

resampler.SetOutputSpacing( fixedImage.GetSpacing() )
resampler.SetOutputOrigin(  fixedImage.GetOrigin()  )
resampler.SetOutputDirection(  fixedImage.GetDirection()  )
resampler.SetDefaultPixelValue( 100 )

outputCast = itkRescaleIntensityImageFilterIF2IUC2_New()
outputCast.SetOutputMinimum(      0  )
outputCast.SetOutputMaximum(  255  )
outputCast.SetInput(resampler.GetOutput())

#
#  Write the resampled image
#
writer = itkImageFileWriterIUC2_New()

writer.SetFileName( outputImageName )
writer.SetInput( outputCast.GetOutput() )
writer.Update()
print &quot;image registration has been finished&quot;
</code></pre>
<h2 id="usage">Usage</h2>
<h3 id="basic-examples">Basic examples</h3>
<p>Here is a simple python script that reads an image, applies a median image filter (radius of 2 pixels), and writes the resulting image in a file.</p>
<pre><code>#!/usr/bin/env python

import itk
import sys

input_filename = sys.argv[1]
output_filename = sys.argv[2]

image = itk.imread(input_filename)
median = itk.MedianImageFilter.New(image, Radius = 2)
itk.imwrite(median, output_filename)
</code></pre>
<p>There are two ways to instantiate filters with ITKPython:</p>
<ul>
<li>Implicit (recommended): ITK type information is automatically detected from the data. Typed filter objects and images are implicitly created.</li>
</ul>
<pre><code>
 # Use `ImageFileReader` instead of the wrapping function `imread` to illustrate this example.
reader = itk.ImageFileReader.New(FileName=input_filename)
# Here we specify the filter input explicitly
median = itk.MedianImageFilter.New(Input=reader.GetOutput())
# Same as above but shortened. `Input` does not have to be specified.
median = itk.MedianImageFilter.New(reader.GetOutput())
# Same as above. `.GetOutput()` does not have to be specified.
median = itk.MedianImageFilter.New(reader)

*   Explicit: This can be useful if a filter cannot automatically select the type information (e.g. &lt;cite&gt;CastImageFilter&lt;/cite&gt;), or to detect type mismatch errors which can lead to cryptic error messages.

Explicit instantiation of median image filter:

 # Use `ImageFileReader` instead of the wrapping function `imread` to illustrate this example.
reader = itk.ImageFileReader.New(FileName=input_filename)
# Here we specify the filter input explicitly
median = itk.MedianImageFilter.New(Input=reader.GetOutput())
# Same as above but shortened. `Input` does not have to be specified.
median = itk.MedianImageFilter.New(reader.GetOutput())
# Same as above. `.GetOutput()` does not have to be specified.
median = itk.MedianImageFilter.New(reader)

Explicit instantiation of cast image filter:

 image = itk.imread(input_filename)
InputType = type(image)
# Find input image dimension
input_dimension = image.GetImageDimension()
# Select float as output pixel type
OutputType = itk.Image[itk.UC, input_dimension]
castFilter = itk.CastImageFilter[InputType, OutputType].New()
castFilter.SetInput(image)
itk.imwrite(castFilter, output_filename)
</code></pre>
<h3 id="itk-python-types">ITK Python types</h3>
<colgroup><col width="51%"><col width="49%"></colgroup>
| C++ type | Python type |
| --- | --- |
| float | itk.F |
| double | itk.D |
| unsigned char | itk.UC |
| std::complex<float> | itk.complex[itk.F] |
<p>This list is not exhaustive and is only presented to illustrate the type names. The complete list of types can be found in the <a href="https://itk.org/ITKSoftwareGuide/html/Book1/ITKSoftwareGuide-Book1ch9.html#x48-1530009.5">ITK Software Guide</a>.</p>
<p>Types can also be obtained from their name in the C programming language:</p>
<p>itk.F == itk.ctype('float') # True</p>
<h3 id="instantiate-an-itk-object">Instantiate an ITK object</h3>
<p>There are two types of ITK objects. Most ITK objects (images, filters, adapters, …) are instantiated the following way:</p>
<p>InputType = itk.Image[itk.F,3]
OutputType = itk.Image[itk.F,3]
median = itk.MedianImageFilter[InputType, OutputType].New()</p>
<p>Some objects (matrix, vector, RGBPixel, …) do not require the attribute <cite>.New()</cite> to be added to instantiate them:</p>
<p>pixel = itk.RGBPixel<a href="">itk.D</a></p>
<p>In case of doubt, look at the attributes of the object you are trying to instantiate.</p>
<h2 id="mixing-itk-and-numpy">Mixing ITK and NumPy</h2>
<p>A common use case for using ITK in Python is to mingle NumPy and ITK operations on raster data. ITK provides a large number of I/O image formats and several sophisticated image processing algorithms not available in any other packages. The ability to intersperse that with numpy special purpose hacking provides a great tool for rapid prototyping.</p>
<p>The following script shows how to integrate NumPy and ITK:</p>
<pre><code>
 import itk
import numpy as np

# Read input image
itk_image = itk.imread(input_filename)

# Run filters on itk.Image

# View only of itk.Image, data is not copied
np_view = itk.GetArrayViewFromImage(itk_image)

# Copy of itk.Image, data is copied
np_copy = itk.GetArrayFromImage(itk_image)

# Do numpy stuff

# Convert back to itk, view only, data is not copied
itk_np_view = itk.GetImageViewFromArray(np_copy)

# Convert back to itk, data is copied
itk_np_copy = itk.GetImageFromArray(np_copy)

# Save result
itk.imwrite(itk_np_view, output_filename)
</code></pre>
<p>Similar functions are available to work with VNL vector and matrices:</p>
<pre><code> # Vnl matrix from array
arr = np.zeros([3,3], np.uint8)
matrix = itk.GetVnlMatrixFromArray(arr)

# Array from Vnl matrix
arr = itk.GetArrayFromVnlMatrix(matrix)

# Vnl vector from array
vec = np.zeros([3], np.uint8)
vnl_vector = itk.GetVnlVectorFromArray(vec)

# Array from Vnl vector
vec = itk.GetArrayFromVnlVector(vnl_vector)
</code></pre>
<h2 id="examples">Examples</h2>
<p>Examples can be found in the <a href="https://itk.org/ITKExamples/src/index.html">ITKExamples project</a>.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[slicer 中删除Node]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/4xnWUQLST</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/4xnWUQLST">
        </link>
        <updated>2020-06-12T05:22:19.000Z</updated>
        <content type="html"><![CDATA[<p>如何利用slicer中删除相关的数据</p>
<pre><code>RemoveNodeFromScene(shNode)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何在slicer中沿着一条弧线移动]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y">
        </link>
        <updated>2020-06-11T03:17:34.000Z</updated>
        <content type="html"><![CDATA[<p>如何在slicer中沿着一条弧线移动</p>
<pre><code># Load sample volume
import SampleData
sampleDataLogic = SampleData.SampleDataLogic()
mrHead = sampleDataLogic.downloadMRHead()

# Create transform and apply to sample volume
transformNode = slicer.vtkMRMLTransformNode()
slicer.mrmlScene.AddNode(transformNode)
mrHead.SetAndObserveTransformNodeID(transformNode.GetID())

# How to move a volume along a trajectory using a transform:
import time
import math
transformMatrix = vtk.vtkMatrix4x4()
for xPos in range(-30,30):
  transformMatrix.SetElement(0,3, xPos)
  transformMatrix.SetElement(1,3, math.sin(xPos)*10)
  transformNode.SetMatrixTransformToParent(transformMatrix)
  slicer.app.processEvents()
  time.sleep(0.02)
# Note: for longer animations use qt.QTimer.singleShot(100, callbackFunction)
# instead of a for loop.
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[更改点的颜色]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0">
        </link>
        <updated>2020-06-11T03:05:28.000Z</updated>
        <content type="html"><![CDATA[<p>更改点的颜色要注意了
错误的代码：</p>
<pre><code>markupNode = getNode('MarkupsCurve')
displayNode = markupNode.GetDisplayNode()
displayNode.SetColor([0,0,0])
</code></pre>
<p>注意：Markups have Color and SelectedColor properties. SelectedColor is used if all control points are in “selected” state, which is the default. So, in most cases you want to use SetSelectedColor.</p>
]]></content>
    </entry>
</feed>