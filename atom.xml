<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://lizhenzhublog.github.io/HTML</id>
    <title>Li Zhenzhu, Ph.D</title>
    <updated>2020-06-12T05:23:06.491Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://lizhenzhublog.github.io/HTML"/>
    <link rel="self" href="https://lizhenzhublog.github.io/HTML/atom.xml"/>
    <subtitle>Binzhou Medical University Hospital, Email: timeanddoctor@gmail.com.</subtitle>
    <logo>https://lizhenzhublog.github.io/HTML/images/avatar.png</logo>
    <icon>https://lizhenzhublog.github.io/HTML/favicon.ico</icon>
    <rights>All rights reserved 2020, Li Zhenzhu, Ph.D</rights>
    <entry>
        <title type="html"><![CDATA[slicer 中删除Node]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/4xnWUQLST</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/4xnWUQLST">
        </link>
        <updated>2020-06-12T05:22:19.000Z</updated>
        <content type="html"><![CDATA[<p>如何利用slicer中删除相关的数据</p>
<pre><code>RemoveNodeFromScene(shNode)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何在slicer中沿着一条弧线移动]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/ULaNbZC1y">
        </link>
        <updated>2020-06-11T03:17:34.000Z</updated>
        <content type="html"><![CDATA[<p>如何在slicer中沿着一条弧线移动</p>
<pre><code># Load sample volume
import SampleData
sampleDataLogic = SampleData.SampleDataLogic()
mrHead = sampleDataLogic.downloadMRHead()

# Create transform and apply to sample volume
transformNode = slicer.vtkMRMLTransformNode()
slicer.mrmlScene.AddNode(transformNode)
mrHead.SetAndObserveTransformNodeID(transformNode.GetID())

# How to move a volume along a trajectory using a transform:
import time
import math
transformMatrix = vtk.vtkMatrix4x4()
for xPos in range(-30,30):
  transformMatrix.SetElement(0,3, xPos)
  transformMatrix.SetElement(1,3, math.sin(xPos)*10)
  transformNode.SetMatrixTransformToParent(transformMatrix)
  slicer.app.processEvents()
  time.sleep(0.02)
# Note: for longer animations use qt.QTimer.singleShot(100, callbackFunction)
# instead of a for loop.
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[更改点的颜色]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/bEUc0fMT0">
        </link>
        <updated>2020-06-11T03:05:28.000Z</updated>
        <content type="html"><![CDATA[<p>更改点的颜色要注意了
错误的代码：</p>
<pre><code>markupNode = getNode('MarkupsCurve')
displayNode = markupNode.GetDisplayNode()
displayNode.SetColor([0,0,0])
</code></pre>
<p>注意：Markups have Color and SelectedColor properties. SelectedColor is used if all control points are in “selected” state, which is the default. So, in most cases you want to use SetSelectedColor.</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Slicer的VMTK扩展]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/T7OLytblD</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/T7OLytblD">
        </link>
        <updated>2020-06-11T02:58:48.000Z</updated>
        <summary type="html"><![CDATA[<p>此扩展使3D Slicer（www.slicer.org）中提供了血管建模工具包（VMTK，http：//www.vmtk.org/）。功能包括血管树分割和中心线提取。此处提供了有关血管分割和中心线提取的简短演示视频：https : //youtu.be/caEuwJ7pCWs</p>
]]></summary>
        <content type="html"><![CDATA[<p>此扩展使3D Slicer（www.slicer.org）中提供了血管建模工具包（VMTK，http：//www.vmtk.org/）。功能包括血管树分割和中心线提取。此处提供了有关血管分割和中心线提取的简短演示视频：https : //youtu.be/caEuwJ7pCWs</p>
<!-- more --> 
<h1 id="安装">安装</h1>
<p>VMTK扩展可用于最新的3D Slicer版本（Slicer-4.10及更高版本）。安装3D Slicer，启动3D Slicer，然后在扩展管理器中安装SlicerVMTK扩展。</p>
<h1 id="用法">用法</h1>
<p>SlicerVMTK扩展提供以下模块-在模块列表的“血管建模工具包”类别中列出。</p>
<h2 id="血管过滤">血管过滤</h2>
<p>图像处理操作可增加管状结构的亮度并抑制其他形状（板和斑点）。该模块可用于预处理图像数据，以使血管分割更容易。</p>
<h2 id="水平集细分">水平集细分</h2>
<p>该模块可以根据图像分割血管树的单个血管分支（可以使用未处理或经过血管过滤的血管）。</p>
<h2 id="中心线计算">中心线计算</h2>
<p>从输入模型节点确定容器树中的中心线。单击“预览”按钮以快速验证输入模型和近似中心线计算。单击“开始”按钮进行完整的网络分析和所有输出的计算。</p>
<p>所需输入：</p>
<p>血管树模型：可以是任何树结构（不仅是血管树，还可以是气道等），可以在“段编辑器”模块中创建，也可以使用“级别集分割”模块创建。如果使用了细分编辑器，则必须通过在“数据”模块中的细分上单击鼠标右键，然后选择“将可见细分导出到模型”，将细分节点导出到模型节点。
起点：包含单个点的标记基准节点，应将其放置在树的分支处
输出：</p>
<p>中心线模型：网络提取结果（无分支提取和合并）。点包含中心线点，“半径”点数据包含每个点的最大内切球半径。
中心线端点：找到的起点和所有检测到的分支端点的坐标。
Voronoi模型（可选）：输入模型的中间表面（中间表面包含的点与最近的表面点的距离等于q）
曲线树角色（可选）：如果选择了标记曲线节点，则将从提取和合并的分支中创建曲线节点的层次结构。每个分支的CellId将保存到节点的名称中（也保存为节点属性），该名称可用于与中心线属性表的CellId列进行交叉引用。
中心线属性（可选）：如果选择了表节点，则将为所有提取和合并的分支计算分支长度，平均半径，曲率，扭转和曲折度。</p>
<h1 id="高级功能">高级功能：</h1>
<h2 id="提取中心线的点坐标和半径">提取中心线的点坐标和半径</h2>
<pre><code>c = getNode('CenterlineComputationModel')
points = slicer.util.arrayFromModelPoints(c)
radii = slicer.util.arrayFromModelPointData(c, 'Radius')
for i, radius in enumerate(radii):
  print(&quot;Point {0}: position={1}, radius={2}&quot;.format(i, points[i], radius))
</code></pre>
<h2 id="把中心线的点和线转换为三维的vtk">把中心线的点和线转换为三维的vtk</h2>
<pre><code>centerlineModel = getNode('CenterlineComputationModel')
centerlinePoly = centerlineModel.GetPolyData()

# Get first point position:
print(centerlinePoly.GetPoints().GetPoint(0))

# Get point IDs of the first line segment
pointIds = vtk.vtkIdList()
centerlinePoly.GetLines().GetCell(0, pointIds)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D slicer 提取STL中心线]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/qw9_t-Cwj</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/qw9_t-Cwj">
        </link>
        <updated>2020-06-11T02:52:44.000Z</updated>
        <content type="html"><![CDATA[<p>可以利用VMTK扩展</p>
<pre><code>reader = vmtkscripts.vmtkSurfaceReader()
reader.InputFileName = ‘input.stl’
reader.Execute()

# network extraction
networkExtraction = vmtkscripts.vmtkNetworkExtraction()
networkExtraction.Surface = reader.Surface
networkExtraction.Execute()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Python+OpenCV录制双目摄像头视频]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/dL-YnYF3i</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/dL-YnYF3i">
        </link>
        <updated>2020-04-22T12:52:47.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>第一个例子
说起来录制视频，我们可能有很多的软件，但是比较坑的是，好像很少的软件支持能够同时录制两个摄像头的视频，于是我们用python自己写一个。要是OpenCV+python
https://github.com/anonymouslycn/bjtu_BinocularCameraRecord</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>第一个例子
说起来录制视频，我们可能有很多的软件，但是比较坑的是，好像很少的软件支持能够同时录制两个摄像头的视频，于是我们用python自己写一个。要是OpenCV+python
https://github.com/anonymouslycn/bjtu_BinocularCameraRecord</li>
</ul>
<!-- more -->
<pre><code>import cv2
import numpy as np
from PyQt5.QtWidgets import (QMainWindow, QApplication, QFileDialog)
import threading
import threadpool 
from CvPyGui import ImageCvQtContainer
from CvPyGui.ui import gui


#UI

class Image(QWidget):
 &quot;&quot;&quot;Common base for the images&quot;&quot;&quot;
 
 def __init__(self, name, label):
 super().__init__()
 
 # Label (frame) where the original image will be located, with scaling
 self.frame_lbl = label
 
 def updateImage(self, opencv_rgb_image):
 
 self.cv_img_rgb = opencv_rgb_image
 
 height, width, channel = self.cv_img_rgb.shape
 bytesPerLine = 3 * width
 self.q_image = QImage(self.cv_img_rgb.data, width,
  height, bytesPerLine, QImage.Format_RGB888)
 
 self.frame_lbl.setPixmap(QPixmap.fromImage(self.q_image))
 
 def saveImage(self):
 # Function for saving the processed image
 
 filter = &quot;Images (*.png *.jpg)&quot;
 
 image_path, _ = QFileDialog.getSaveFileName(self, filter=filter)
 
 cv_img_bgr = cv2.cvtColor(
 self.cv_img_rgb, cv2.COLOR_RGB2BGR)
 cv2.imwrite(image_path, cv_img_bgr)
 
 #获取视频
cap = cv2.VideoCapture(int(text))
cap.set(6 ,cv2.VideoWriter_fourcc('M', 'J', 'P', 'G') );
cap.set(3,w);
cap.set(4,h);
global update1
update1 = 1
global shotmark1
ret, frame = cap.read()


</code></pre>
<p>这样就能够获取到一帧图像了，其中cap.set()函数用来设置相机的参数，本来应该有宏定义的，但是在python里面老是报错，直接用数字替代了，其中3就是获取视频的宽度像素，4是高度，这个要和摄像头手册上的参数一致。一般的Webcam有两种图像获取格式：一种是YUV2格式这种事10bit回传的数据，理论上质量更好，但是有个很大的问题是分辨率高的时候，帧率就会变得十分低。另一种格式是MJPEG格式，这个是使用了压缩技术得到的视频流。通过这个格式，手册上说在1920x1080分辨率下都能获得30fps的表现，而YUV2只有5fps（后来发现，这个就是坑爹的，信了就怪了）。cap.set(6 ,cv2.VideoWriter_fourcc(‘M', ‘J', ‘P', ‘G') );这个参数就是使用MJPEG格式来读取摄像头的数据。</p>
<p>多线程</p>
<p>刚才我们呢也提到了，cap.read()这个函数是获取到了一帧图像，但是呢。我们要的是动画啊，要是写个循环的话，又会吧进程卡死在循环中，照成假死的状态，所以对于图像的绘制，一定要使用多线程技术。在这里我不仅要吐槽一下了。学了好多年计算机，讲了很多串行算法和编程，一讲到多线程，无非就是打印个Hello World！，根本就没有什么实践，理论倒是学了很多，感觉用的时候头真的好大！</p>
<p>其实这里的多线程也没有什么是吧，就是起调一下。但是要注意的是要控制线程的退出，在python这个我引入的多线程包里面，贼坑的是没有外界控制线程退出的办法！所以，我设置了一个全局变量，使用判断全局变量的值来判断是否让子线程继续下去。</p>
<p>结尾</p>
<p>实际上，还有分辨率/帧率设置功能呢，只不过懒得写了！！！</p>
<ul>
<li>另一例子</li>
</ul>
<pre><code>import cv2

cap = cv2.VideoCapture(0) #默认大小
# cap.set(3,1080)
# cap.set(4,720)
cv2.namedWindow(cv2.WINDOW_NORMAL)#任意调节大小

#显示
import cv2

# 初始化摄像头
cap = cv2.VideoCapture(0)

while cap.isOpened():
    # 采集一帧一帧的图像数据
    isSuccess,frame = cap.read()
    # 实时的将采集到的数据显示到界面上
    if isSuccess:
        cv2.imshow(&quot;My Capture&quot;,frame)
    # 实现按下“q”键退出程序
    if cv2.waitKey(1)&amp;0xFF == ord('q'):
        break

# 释放摄像头资源
cap.release()
cv2.destoryAllWindows()
</code></pre>
<p>录制视频</p>
<pre><code># coding:utf-8
import sys

reload(sys)
sys.setdefaultencoding('utf8')
#    __author__ = '郭 璞'
#    __date__ = '2016/9/7'
#    __Desc__ = 使用Python借助opencv实现对图像的读取，写入

import cv2
import numpy as np
# 选取摄像头，0为笔记本内置的摄像头，1,2···为外接的摄像头
cap = cv2.VideoCapture(0)
# cap.set(3,1080)
# cap.set(4,720)

# 为保存视频做准备
fourcc = cv2.cv.CV_FOURCC(&quot;D&quot;, &quot;I&quot;, &quot;B&quot;, &quot; &quot;)
# 第三个参数则是镜头快慢的，20为正常，小于二十为慢镜头
out = cv2.VideoWriter('output2.avi', fourcc,3.0,(640,480))
while True:
    # 一帧一帧的获取图像
    ret,frame = cap.read()
    if ret == True:
        frame = cv2.flip(frame, 1)
        # 在帧上进行操作
        # gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
        # 开始保存视频
        out.write(frame)
        # 显示结果帧
        cv2.imshow(&quot;frame&quot;, frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('q'):
            break
    else:
        break
# 释放摄像头资源
cap.release()
out.release()
cv2.destroyAllWindows()
</code></pre>
<ul>
<li>另一个例子，我电脑上可以实现的</li>
</ul>
<pre><code>import numpy as np
import cv2

cap = cv2.VideoCapture(0)

fourcc = cv2.VideoWriter_fourcc(*'XVID')
#out = cv2.VideoWriter('output.mp4',fourcc, 30.0, (640,480))  #设置分辨率


# 获取捕获的分辨率
# propId可以直接写数字，也可以用OpenCV的符号表示
width, height = cap.get(3), cap.get(4)
print(width, height)
out = cv2.VideoWriter('output.mp4',fourcc, 30.0, (int(width),int(height)))
'''
# 以原分辨率的一倍来捕获
cap.set(cv2.CAP_PROP_FRAME_WIDTH, width * 2)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height * 2)
'''


while(cap.isOpened()):
    ret, frame = cap.read()
    if ret==True:
        frame = cv2.flip(frame,1)
#        out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))   将视频转换为灰色的源
        out.write(frame)

        cv2.imshow('frame', frame)
        if cv2.waitKey(1) &amp; 0xFF == ord('1'):
            break
    else:
        break

cap.release()
out.release()
cv2.destroyAllWindows()

</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[如何将视频转换为图片]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/dx7xPaKP-</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/dx7xPaKP-">
        </link>
        <updated>2020-04-22T11:30:05.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>如何将视频转换为图片</li>
</ul>
<p>更具根据就</p>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>如何将视频转换为图片</li>
</ul>
<p>更具根据就</p>
<!-- more -->
<pre><code>import numpy as np
import cv2, PIL, os
from cv2 import aruco
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
%matplotlib nbagg

workdir = &quot;../Aruco/data/calib_tel_ludo2/&quot;
name = &quot;VID_20180406_104312.mp4&quot;
rootname = name.split(&quot;.&quot;)[0]
cap = cv2.VideoCapture(workdir + name)
counter = 0
each = 10
length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
for i in range(length):
    ret, frame = cap.read()
    if i % each == 0: cv2.imwrite(workdir + rootname + &quot;_{0}&quot;.format(i) + &quot;.png&quot;, frame)

cap.release()

os.listdir(&quot;../Aruco/data/calib_tel_ludo/&quot;)
int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[另一个完整的例子]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/QAHUqM6yQ</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/QAHUqM6yQ">
        </link>
        <updated>2020-04-22T11:10:21.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>
<ol>
<li>生成棋盘并且打印</li>
</ol>
</li>
</ul>
<pre><code>import numpy as np
import cv2, PIL, os
from cv2 import aruco
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
%matplotlib nbagg

#创建棋盘
workdir = &quot;./workdir/&quot;
aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
board = aruco.CharucoBoard_create(7, 5, 1, .8, aruco_dict)
imboard = board.draw((2000, 2000))
cv2.imwrite(workdir + &quot;chessboard.tiff&quot;, imboard)
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.imshow(imboard, cmap = mpl.cm.gray, interpolation = &quot;nearest&quot;)
ax.axis(&quot;off&quot;)
plt.show()

#通过相机不同角度拍照

datadir = &quot;../../data/calib_tel_ludo/&quot;
images = np.array([datadir + f for f in os.listdir(datadir) if f.endswith(&quot;.png&quot;) ])
order = np.argsort([int(p.split(&quot;.&quot;)[-2].split(&quot;_&quot;)[-1]) for p in images])
images = images[order]
images
#读取一个照片并展示
im = PIL.Image.open(images[0])
fig = plt.figure()
ax = fig.add_subplot(1,1,1)
plt.imshow(im)
#ax.axis('off')
plt.show()
</code></pre>
<ul>
<li>2.现在，可以使用棋盘上的所有图像进行相机校准。两个功能是必需的：</li>
</ul>
<p>第一个将检测所有图像上的标记和。
第二秒将继续检测到的标记以估计相机校准数据。</p>
<pre><code>def read_chessboards(images):
    &quot;&quot;&quot;
    Charuco base pose estimation.
    &quot;&quot;&quot;
    print(&quot;POSE ESTIMATION STARTS:&quot;)
    allCorners = []
    allIds = []
    decimator = 0
    # SUB PIXEL CORNER DETECTION CRITERION
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.00001)

    for im in images:
        print(&quot;=&gt; Processing image {0}&quot;.format(im))
        frame = cv2.imread(im)
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        corners, ids, rejectedImgPoints = cv2.aruco.detectMarkers(gray, aruco_dict)

        if len(corners)&gt;0:
            # SUB PIXEL DETECTION
            for corner in corners:
                cv2.cornerSubPix(gray, corner,
                                 winSize = (3,3),
                                 zeroZone = (-1,-1),
                                 criteria = criteria)
            res2 = cv2.aruco.interpolateCornersCharuco(corners,ids,gray,board)
            if res2[1] is not None and res2[2] is not None and len(res2[1])&gt;3 and decimator%1==0:
                allCorners.append(res2[1])
                allIds.append(res2[2])

        decimator+=1

    imsize = gray.shape
    return allCorners,allIds,imsize


def calibrate_camera(allCorners,allIds,imsize):
    &quot;&quot;&quot;
    Calibrates the camera using the dected corners.
    &quot;&quot;&quot;
    print(&quot;CAMERA CALIBRATION&quot;)

    cameraMatrixInit = np.array([[ 1000.,    0., imsize[0]/2.],
                                 [    0., 1000., imsize[1]/2.],
                                 [    0.,    0.,           1.]])

    distCoeffsInit = np.zeros((5,1))
    flags = (cv2.CALIB_USE_INTRINSIC_GUESS + cv2.CALIB_RATIONAL_MODEL + cv2.CALIB_FIX_ASPECT_RATIO)
    #flags = (cv2.CALIB_RATIONAL_MODEL)
    (ret, camera_matrix, distortion_coefficients0,
     rotation_vectors, translation_vectors,
     stdDeviationsIntrinsics, stdDeviationsExtrinsics,
     perViewErrors) = cv2.aruco.calibrateCameraCharucoExtended(
                      charucoCorners=allCorners,
                      charucoIds=allIds,
                      board=board,
                      imageSize=imsize,
                      cameraMatrix=cameraMatrixInit,
                      distCoeffs=distCoeffsInit,
                      flags=flags,
                      criteria=(cv2.TERM_CRITERIA_EPS &amp; cv2.TERM_CRITERIA_COUNT, 10000, 1e-9))

    return ret, camera_matrix, distortion_coefficients0, rotation_vectors, translation_vectors


#读取所有的照片		
allCorners,allIds,imsize=read_chessboards(images)

#标定相机
%time
ret, mtx, dist, rvecs, tvecs = calibrate_camera(allCorners,allIds,imsize)


</code></pre>
<ul>
<li>3.检验标记结果</li>
</ul>
<pre><code>#Check calibration results
i=20 # 选择打开哪一个照片
plt.figure()
frame = cv2.imread(images[i])
img_undist = cv2.undistort(frame,mtx,dist,None)
plt.subplot(1,2,1)
plt.imshow(frame)
plt.title(&quot;Raw image&quot;)
plt.axis(&quot;off&quot;)
plt.subplot(1,2,2)
plt.imshow(img_undist)
plt.title(&quot;Corrected image&quot;)
plt.axis(&quot;off&quot;)
plt.show()
</code></pre>
<ul>
<li>4.开始使用相机并且预估3D</li>
</ul>
<pre><code>#读取照片
frame = cv2.imread(&quot;../../data/IMG_20180406_095219.jpg&quot;)
#frame = cv2.undistort(src = frame, cameraMatrix = mtx, distCoeffs = dist)
plt.figure()
plt.imshow(frame, interpolation = &quot;nearest&quot;)
plt.show()

#处理
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
parameters =  aruco.DetectorParameters_create()
corners, ids, rejectedImgPoints = aruco.detectMarkers(gray, aruco_dict,
                                                      parameters=parameters)
# SUB PIXEL DETECTION
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 100, 0.0001)
for corner in corners:
    cv2.cornerSubPix(gray, corner, winSize = (3,3), zeroZone = (-1,-1), criteria = criteria)

frame_markers = aruco.drawDetectedMarkers(frame.copy(), corners, ids)

print(corners)

#画出图片
plt.figure()
plt.imshow(frame_markers, interpolation = &quot;nearest&quot;)

plt.show()
</code></pre>
<ul>
<li>5.添加3轴标识</li>
</ul>
<pre><code>size_of_marker =  0.0285 # side lenght of the marker in meter
rvecs,tvecs = aruco.estimatePoseSingleMarkers(corners, size_of_marker , mtx, dist)

#添加轴
length_of_axis = 0.1
imaxis = aruco.drawDetectedMarkers(frame.copy(), corners, ids)

for i in range(len(tvecs)):
    imaxis = aruco.drawAxis(imaxis, mtx, dist, rvecs[i], tvecs[i], length_of_axis)
		
plt.figure()
plt.imshow(imaxis)
plt.grid()
plt.show()

#表格展示移动参数
data = pd.DataFrame(data = tvecs.reshape(len(tvecs),3), columns = [&quot;tx&quot;, &quot;ty&quot;, &quot;tz&quot;],
                    index = ids.flatten())
data.index.name = &quot;marker&quot;
data.sort_index(inplace= True)
print(data)
#表格展示旋转参数
datar = pd.DataFrame(data = tvecs.reshape(len(rvecs),3), columns = [&quot;rx&quot;, &quot;ry&quot;, &quot;rz&quot;],
                    index = ids.flatten())
datar.index.name = &quot;marker&quot;
datar.sort_index(inplace= True)
np.degrees(datar)
</code></pre>
<ul>
<li>6.后期处理相关的数据</li>
</ul>
<pre><code>v = data.loc[3:6].values
((v[1:] - v[:-1])**2).sum(axis = 1)**.5
cv2.Rodrigues(rvecs[0], np.zeros((3,3)))
fig = plt.figure()
#ax = fig.add_subplot(111, projection='3d')
ax = fig.add_subplot(1,2,1)
ax.set_aspect(&quot;equal&quot;)
plt.plot(data.tx, data.ty, &quot;or-&quot;)
plt.grid()
ax = fig.add_subplot(1,2,2)
plt.imshow(imaxis, origin = &quot;upper&quot;)
plt.plot(np.array(corners)[:, 0, 0,0], np.array(corners)[:, 0, 0,1], &quot;or&quot;)
plt.show()
</code></pre>
<ul>
<li>7.开始定义在图片中的位置</li>
</ul>
<pre><code>a = np.arange(50)
import pickle
f = open(&quot;truc.pckl&quot;, &quot;wb&quot;)
pickle.dump(a, f)
f.close()

f = open(&quot;truc.pckl&quot;, &quot;rb&quot;)
b = pickle.load(f)
b == a

corners = np.array(corners)
data2 = pd.DataFrame({&quot;px&quot;: corners[:, 0, 0, 1],
                      &quot;py&quot;: corners[:, 0, 0, 0]}, index = ids.flatten())
data2.sort_index(inplace=True)
print(data2)

m0 = data2.loc[0]
m43 = data2.loc[43]
d01 = ((m0 - m43).values**2).sum()**.5
d = 42.5e-3 * (3.5**2 + 4.5**2)**.5
factor = d / d01
data2[&quot;x&quot;] = data2.px * factor
data2[&quot;y&quot;] = data2.py * factor
((data2[[&quot;x&quot;, &quot;y&quot;]].loc[11] - data2[[&quot;x&quot;, &quot;y&quot;]].loc[0]).values**2).sum()**.5
c = np.array(corners).astype(np.float64).reshape(44,4,2)
(((c[:, 1:] - c[:, :-1])**2).sum(axis = 2)**.5).mean(axis =1)
</code></pre>
<ul>
<li>9.利用help函数查看帮助文档</li>
</ul>
<pre><code>help(cv2.aruco.detectMarkers)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[机器视觉2 创建标记并识别]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/O_TkJInMU</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/O_TkJInMU">
        </link>
        <updated>2020-04-22T11:00:59.000Z</updated>
        <content type="html"><![CDATA[<ul>
<li>创建</li>
</ul>
<pre><code>import numpy as np
import cv2, PIL
from cv2 import aruco
import matplotlib.pyplot as plt
import matplotlib as mpl
import pandas as pd
%matplotlib nbagg


aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)

fig = plt.figure()
nx = 4
ny = 3
for i in range(1, nx*ny+1):
    ax = fig.add_subplot(ny,nx, i)
    img = aruco.drawMarker(aruco_dict,i, 700)
    plt.imshow(img, cmap = mpl.cm.gray, interpolation = &quot;nearest&quot;)
    ax.axis(&quot;off&quot;)

plt.savefig(&quot;_data/markers.pdf&quot;)  #保存为pdf
plt.show()

#通过打印机打印，裁剪后拍照
#读取照片
frame = cv2.imread(&quot;_data/aruco_photo.jpg&quot;)
plt.figure()
plt.imshow(frame)
plt.show()

#追踪
%%time
gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
aruco_dict = aruco.Dictionary_get(aruco.DICT_6X6_250)
parameters =  aruco.DetectorParameters_create()
corners, ids, rejectedImgPoints = aruco.detectMarkers(gray, aruco_dict, parameters=parameters)
frame_markers = aruco.drawDetectedMarkers(frame.copy(), corners, ids)

#把追踪的结果图片展示
plt.figure()
plt.imshow(frame_markers)
for i in range(len(ids)):
    c = corners[i][0]
    plt.plot([c[:, 0].mean()], [c[:, 1].mean()], &quot;o&quot;, label = &quot;id={0}&quot;.format(ids[i]))
plt.legend()
plt.show()
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[机器视觉]]></title>
        <id>https://lizhenzhublog.github.io/HTML/post/XMhGzY5TR</id>
        <link href="https://lizhenzhublog.github.io/HTML/post/XMhGzY5TR">
        </link>
        <updated>2020-04-22T10:23:31.000Z</updated>
        <content type="html"><![CDATA[<p><strong>Aruco</strong></p>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/Projet+calibration-Paul.html">检查校准</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/Projet+calibration-Paul.html#use-of-camera-calibration-to-estimate-3d-translation-and-rotation-of-each-marker-on-a-scene">使用相机校准来估计场景中每个标记的3D平移和旋转</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/Projet+calibration-Paul.html#post-processing">后期处理</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/Projet+calibration-Paul.html#result">结果</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/Projet+calibration-Paul.html#add-local-axis-on-each-maker">在每个制造商上添加局部轴</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics.html">ARUCO标记：基础知识</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics.html#marker-creation">1：标记创建</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics.html#print-cut-stick-and-take-a-picture">2：打印，剪切，粘贴并拍照</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics.html#post-processing">3：后期处理</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics.html#results">4：结果</a></li>
</ul>
</li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics_video.html">ARUCO标记：基础知识</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics_video.html#marker-creation">1：标记创建</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics_video.html#print-cut-stick-and-take-a-picture">2：打印，剪切，粘贴并拍照</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics_video.html#post-processing">3：后期处理</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/aruco_basics_video.html#results">4：结果</a></li>
</ul>
</li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/sandbox.html">沙盒</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/ludovic.html">鲁多维奇</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html">使用CHARUCO进行相机校准</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#camera-pose-estimation-using-charuco-chessboard">2.使用CHARUCO国际象棋棋盘估算相机姿势</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#check-calibration-results">检查校准结果</a></li>
</ul>
</li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#use-of-camera-calibration-to-estimate-3d-translation-and-rotation-of-each-marker-on-a-scene">3。使用相机校准来估计场景中每个标记的3D平移和旋转</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#post-processing">后期处理</a></li>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#results">结果</a>
<ul>
<li><a href="https://mecaruco2.readthedocs.io/en/latest/notebooks_rst/Aruco/sandbox/ludovic/aruco_calibration_rotation.html#add-local-axis-on-each-marker">在每个标记上添加局部轴</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
</feed>